{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_FILE = 'data/ru-be-train.txt'\n",
    "TEST_FILE = 'data/ru-be-test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lib.utils import Alphabet\n",
    "\n",
    "\n",
    "ru = Alphabet()\n",
    "be = Alphabet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lib.utils import load_pair_dataset\n",
    "\n",
    "X, Y = load_pair_dataset(TRAIN_FILE, ru, be)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, val_X, train_Y, val_Y = train_test_split(X, Y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lib.models import SimpleGRUSupervisedSeq2Seq\n",
    "\n",
    "model = SimpleGRUSupervisedSeq2Seq(ru, be, 65, 256)\n",
    "opt = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_src_words = [ru.index2letter(x, with_start_end=False) for x in val_X]\n",
    "val_trg_words = [be.index2letter(y, with_start_end=False) for y in val_Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RU_GENERATOR_CHECKPOINTS = './checkpoints/ru_generators_checkpoints'\n",
    "\n",
    "! mkdir -p {RU_GENERATOR_CHECKPOINTS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 iter: 49 loss: 0.18749146784438972\n",
      "epoch: 0 iter: 99 loss: 0.1925157384710806\n",
      "epoch: 0 iter: 149 loss: 0.1971139955831735\n",
      "epoch: 0 iter: 199 loss: 0.19273636193160368\n",
      "epoch: 0 iter: 249 loss: 0.1800166141711831\n",
      "epoch: 0 iter: 299 loss: 0.17534948138655812\n",
      "epoch: 0 iter: 349 loss: 0.16808016918203736\n",
      "epoch: 0 iter: 399 loss: 0.19870731300857897\n",
      "epoch: 0 iter: 449 loss: 0.18669938045001172\n",
      "epoch: 0 iter: 499 loss: 0.17915205913272433\n",
      "epoch: 0 iter: 549 loss: 0.17623469553709434\n",
      "epoch: 0 iter: 599 loss: 0.17824407318836993\n",
      "epoch: 0 iter: 649 loss: 0.19722229769145933\n",
      "epoch: 0 iter: 699 loss: 0.19358745286883056\n",
      "epoch: 0 iter: 749 loss: 0.21682424548138\n",
      "epoch: 0 iter: 799 loss: 0.2037420646300935\n",
      "epoch: 0 iter: 849 loss: 0.20755405683640238\n",
      "epoch: 0 iter: 899 loss: 0.21673287047922946\n",
      "epoch: 0 iter: 949 loss: 0.21825829383429313\n",
      "epoch: 0 iter: 999 loss: 0.19722665209180223\n",
      "epoch: 0 iter: 1049 loss: 0.2088876228990186\n",
      "epoch: 0 iter: 1099 loss: 0.21210584177204816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yury/anaconda3/lib/python3.5/site-packages/nltk/translate/bleu_score.py:472: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/home/yury/anaconda3/lib/python3.5/site-packages/nltk/translate/bleu_score.py:472: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/home/yury/anaconda3/lib/python3.5/site-packages/nltk/translate/bleu_score.py:472: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 0 val_score: 0.7993138246330833 time: 444.489666223526\n",
      "epoch: 1 iter: 49 loss: 0.14538353563933032\n",
      "epoch: 1 iter: 99 loss: 0.15838343080189057\n",
      "epoch: 1 iter: 149 loss: 0.16282482438438467\n",
      "epoch: 1 iter: 199 loss: 0.15753384125834352\n",
      "epoch: 1 iter: 249 loss: 0.1541912020099127\n",
      "epoch: 1 iter: 299 loss: 0.14959332736390032\n",
      "epoch: 1 iter: 349 loss: 0.1613433498939563\n",
      "epoch: 1 iter: 399 loss: 0.1618052241266536\n",
      "epoch: 1 iter: 449 loss: 0.17411149292950762\n",
      "epoch: 1 iter: 499 loss: 0.1830906354064941\n",
      "epoch: 1 iter: 549 loss: 0.15706067688288924\n",
      "epoch: 1 iter: 599 loss: 0.18614956338251967\n",
      "epoch: 1 iter: 649 loss: 0.19174604884268717\n",
      "epoch: 1 iter: 699 loss: 0.19625491330518802\n",
      "epoch: 1 iter: 749 loss: 0.21506548780049592\n",
      "epoch: 1 iter: 799 loss: 0.20193613857077772\n",
      "epoch: 1 iter: 849 loss: 0.20712392192739468\n",
      "epoch: 1 iter: 899 loss: 0.20319327611458343\n",
      "epoch: 1 iter: 949 loss: 0.2047374716869786\n",
      "epoch: 1 iter: 999 loss: 0.23052755541578493\n",
      "epoch: 1 iter: 1049 loss: 0.21477495372983607\n",
      "epoch: 1 iter: 1099 loss: 0.22679466390238828\n",
      "\n",
      "epoch: 1 val_score: 0.7933625458591186 time: 469.6091754436493\n",
      "epoch: 2 iter: 49 loss: 0.17502683796483554\n",
      "epoch: 2 iter: 99 loss: 0.18480838178220554\n",
      "epoch: 2 iter: 149 loss: 0.16010634734748305\n",
      "epoch: 2 iter: 199 loss: 0.1647124771033799\n",
      "epoch: 2 iter: 249 loss: 0.17801298803839805\n",
      "epoch: 2 iter: 299 loss: 0.16756422191115097\n",
      "epoch: 2 iter: 349 loss: 0.18552617034437943\n",
      "epoch: 2 iter: 399 loss: 0.15648398619995801\n",
      "epoch: 2 iter: 449 loss: 0.15903129454891132\n",
      "epoch: 2 iter: 499 loss: 0.1882785195035862\n",
      "epoch: 2 iter: 549 loss: 0.1931640089719793\n",
      "epoch: 2 iter: 599 loss: 0.1968880013720525\n",
      "epoch: 2 iter: 649 loss: 0.1857017940519344\n",
      "epoch: 2 iter: 699 loss: 0.17443471650638326\n",
      "epoch: 2 iter: 749 loss: 0.18596227031971813\n",
      "epoch: 2 iter: 799 loss: 0.22270954742837623\n",
      "epoch: 2 iter: 849 loss: 0.23878632631242797\n",
      "epoch: 2 iter: 899 loss: 0.22153899886280468\n",
      "epoch: 2 iter: 949 loss: 0.22608298214222133\n",
      "epoch: 2 iter: 999 loss: 0.24964809046141473\n",
      "epoch: 2 iter: 1049 loss: 0.2114694429624497\n",
      "epoch: 2 iter: 1099 loss: 0.22437835871930426\n",
      "\n",
      "epoch: 2 val_score: 0.7808377095731048 time: 496.935834646225\n",
      "epoch: 3 iter: 49 loss: 0.20789951514426938\n",
      "epoch: 3 iter: 99 loss: 0.19062439291135144\n",
      "epoch: 3 iter: 149 loss: 0.19463197485441008\n",
      "epoch: 3 iter: 199 loss: 0.17988392744685364\n",
      "epoch: 3 iter: 249 loss: 0.17270897541004507\n",
      "epoch: 3 iter: 299 loss: 0.18190024986208386\n",
      "epoch: 3 iter: 349 loss: 0.19816638218410448\n",
      "epoch: 3 iter: 399 loss: 0.18853552634062754\n",
      "epoch: 3 iter: 449 loss: 0.1813220055490299\n",
      "epoch: 3 iter: 499 loss: 0.18876555971123968\n",
      "epoch: 3 iter: 549 loss: 0.19880120302606838\n",
      "epoch: 3 iter: 599 loss: 0.1846797206814844\n",
      "epoch: 3 iter: 649 loss: 0.22148133833589756\n",
      "epoch: 3 iter: 699 loss: 0.22224523700160906\n",
      "epoch: 3 iter: 749 loss: 0.20772725482449333\n",
      "epoch: 3 iter: 799 loss: 0.2110101237957954\n",
      "epoch: 3 iter: 849 loss: 0.1764686837773244\n",
      "epoch: 3 iter: 899 loss: 0.20888401911466298\n",
      "epoch: 3 iter: 949 loss: 0.20544949487226496\n",
      "epoch: 3 iter: 999 loss: 0.24592624956568312\n",
      "epoch: 3 iter: 1049 loss: 0.21001361528286877\n",
      "epoch: 3 iter: 1099 loss: 0.2254204964152266\n",
      "\n",
      "epoch: 3 val_score: 0.7991665819946718 time: 526.6154017448425\n",
      "epoch: 4 iter: 49 loss: 0.1517593360212975\n",
      "epoch: 4 iter: 99 loss: 0.14610178424564746\n",
      "epoch: 4 iter: 149 loss: 0.15455753129050442\n",
      "epoch: 4 iter: 199 loss: 0.16436158090434408\n",
      "epoch: 4 iter: 249 loss: 0.1563295915346274\n",
      "epoch: 4 iter: 299 loss: 0.16186417876494702\n",
      "epoch: 4 iter: 349 loss: 0.1777761748810735\n",
      "epoch: 4 iter: 399 loss: 0.18402723388092068\n",
      "epoch: 4 iter: 449 loss: 0.1866766906486555\n",
      "epoch: 4 iter: 499 loss: 0.16883784046113195\n",
      "epoch: 4 iter: 549 loss: 0.18928547016562064\n",
      "epoch: 4 iter: 599 loss: 0.19029836239005116\n",
      "epoch: 4 iter: 649 loss: 0.19279998015106417\n",
      "epoch: 4 iter: 699 loss: 0.16577948969410955\n",
      "epoch: 4 iter: 749 loss: 0.21877583144766993\n",
      "epoch: 4 iter: 799 loss: 0.1920445167308494\n",
      "epoch: 4 iter: 849 loss: 0.19317870456455105\n",
      "epoch: 4 iter: 899 loss: 0.19497323984132944\n",
      "epoch: 4 iter: 949 loss: 0.19755912722551788\n",
      "epoch: 4 iter: 999 loss: 0.1771152235102055\n",
      "epoch: 4 iter: 1049 loss: 0.21313781885545724\n",
      "epoch: 4 iter: 1099 loss: 0.21146597036455184\n",
      "\n",
      "epoch: 4 val_score: 0.7960093132447283 time: 519.4519951343536\n",
      "epoch: 5 iter: 49 loss: 0.1462697191774026\n",
      "epoch: 5 iter: 99 loss: 0.16438023987439004\n",
      "epoch: 5 iter: 149 loss: 0.16101443986849356\n",
      "epoch: 5 iter: 199 loss: 0.15557138946161683\n",
      "epoch: 5 iter: 249 loss: 0.15176194306189836\n",
      "epoch: 5 iter: 299 loss: 0.17406595608905326\n",
      "epoch: 5 iter: 349 loss: 0.15812841777574804\n",
      "epoch: 5 iter: 399 loss: 0.17805810562806318\n",
      "epoch: 5 iter: 449 loss: 0.17729955674348588\n",
      "epoch: 5 iter: 499 loss: 0.1513780998845215\n",
      "epoch: 5 iter: 549 loss: 0.15597470745685013\n",
      "epoch: 5 iter: 599 loss: 0.18315105043379953\n",
      "epoch: 5 iter: 649 loss: 0.1735264666500911\n",
      "epoch: 5 iter: 699 loss: 0.1915338992928053\n",
      "epoch: 5 iter: 749 loss: 0.1988112171662613\n",
      "epoch: 5 iter: 799 loss: 0.19887476175190097\n",
      "epoch: 5 iter: 849 loss: 0.21059415666243225\n",
      "epoch: 5 iter: 899 loss: 0.20997639516329192\n",
      "epoch: 5 iter: 949 loss: 0.19562961880408541\n",
      "epoch: 5 iter: 999 loss: 0.2336393342216529\n",
      "epoch: 5 iter: 1049 loss: 0.19883913919325014\n",
      "epoch: 5 iter: 1099 loss: 0.2091785184689772\n",
      "\n",
      "epoch: 5 val_score: 0.7903124002124673 time: 534.948855638504\n",
      "epoch: 6 iter: 49 loss: 0.16940708638217686\n",
      "epoch: 6 iter: 99 loss: 0.201091593997124\n",
      "epoch: 6 iter: 149 loss: 0.18880814737055543\n",
      "epoch: 6 iter: 199 loss: 0.19787932826820204\n",
      "epoch: 6 iter: 249 loss: 0.1928926338720717\n",
      "epoch: 6 iter: 299 loss: 0.1928696094766768\n",
      "epoch: 6 iter: 349 loss: 0.16875122224276845\n",
      "epoch: 6 iter: 399 loss: 0.2051022292832866\n",
      "epoch: 6 iter: 449 loss: 0.18941785612112208\n",
      "epoch: 6 iter: 499 loss: 0.2213261385214288\n",
      "epoch: 6 iter: 549 loss: 0.19599003870838305\n",
      "epoch: 6 iter: 599 loss: 0.18732565545462065\n",
      "epoch: 6 iter: 649 loss: 0.19911549614579277\n",
      "epoch: 6 iter: 699 loss: 0.17915386291834737\n",
      "epoch: 6 iter: 749 loss: 0.22646506051926274\n",
      "epoch: 6 iter: 799 loss: 0.18447861601807045\n",
      "epoch: 6 iter: 849 loss: 0.24923819732557626\n",
      "epoch: 6 iter: 899 loss: 0.23827879519941478\n",
      "epoch: 6 iter: 949 loss: 0.24894198306753537\n",
      "epoch: 6 iter: 999 loss: 0.28066977579706787\n",
      "epoch: 6 iter: 1049 loss: 0.26700810661080426\n",
      "epoch: 6 iter: 1099 loss: 0.23801449944012307\n",
      "\n",
      "epoch: 6 val_score: 0.7805420041541131 time: 498.5965530872345\n",
      "epoch: 7 iter: 49 loss: 0.20285419456779388\n",
      "epoch: 7 iter: 99 loss: 0.16826338511849726\n",
      "epoch: 7 iter: 149 loss: 0.17957019043942785\n",
      "epoch: 7 iter: 199 loss: 0.16990355366269755\n",
      "epoch: 7 iter: 249 loss: 0.17372969786938627\n",
      "epoch: 7 iter: 299 loss: 0.18329482790869972\n",
      "epoch: 7 iter: 349 loss: 0.18697280482285714\n",
      "epoch: 7 iter: 399 loss: 0.20358432489889544\n",
      "epoch: 7 iter: 449 loss: 0.20586169466531887\n",
      "epoch: 7 iter: 499 loss: 0.1860750555380265\n",
      "epoch: 7 iter: 549 loss: 0.18826618425601088\n",
      "epoch: 7 iter: 599 loss: 0.19130702405067349\n",
      "epoch: 7 iter: 649 loss: 0.1956883217532397\n",
      "epoch: 7 iter: 699 loss: 0.19692384550983294\n",
      "epoch: 7 iter: 749 loss: 0.19875795072757518\n",
      "epoch: 7 iter: 799 loss: 0.21870456797650945\n",
      "epoch: 7 iter: 849 loss: 0.22791728649523452\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-0bf596508635>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mval_src_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_trg_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcheckpoints_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRU_GENERATOR_CHECKPOINTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmetrics_compute_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;32m/home/yury/diploma/lib/trainer/single_generator_llh_trainer.py\u001b[0m in \u001b[0;36mtrain_generator\u001b[0;34m(model, opt, alph_Y, train_X, train_Y, val_src_words, val_trg_words, checkpoints_folder, metrics_compute_freq, n_epochs)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mlog_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;31m# print(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malph_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yury/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yury/diploma/lib/models/simple_gru_generator.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_sequence, output_sequence)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yury/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yury/diploma/lib/models/simple_gru_generator.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, token, prev_h, encoder_hs, encoder_mask)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;31m# print(attention.shape, embedding.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yury/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yury/diploma/lib/models/multiplicative_attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, decoder_hidden, encoder_hiddens, encoder_mask)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \"\"\"\n\u001b[1;32m     19\u001b[0m         \u001b[0mdecoder_hidden_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mencoder_hiddens_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_hiddens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_hiddens_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden_key\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq2seq_softmax_with_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yury/anaconda3/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mtanh\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    805\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTanh\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \"\"\"\n\u001b[0;32m--> 807\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from lib.trainer import train_generator\n",
    "\n",
    "train_generator(\n",
    "    model, opt, be, \n",
    "    train_X, train_Y, \n",
    "    val_src_words, val_trg_words, \n",
    "    checkpoints_folder=RU_GENERATOR_CHECKPOINTS, \n",
    "    metrics_compute_freq=50, n_epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "best_score_model = \"state_dict_0_0.7993138246330833.pth\"\n",
    "model.load_state_dict(torch.load(os.path.join(RU_GENERATOR_CHECKPOINTS, best_score_model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'федэральным'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.translate(\"федеральным\", with_start_end=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lib.metrics import compute_accuracy\n",
    "\n",
    "#compute_accuracy(model, val_src_words, val_trg_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_words = [ru.index2letter(x, with_start_end=False) for x in train_X[:8000]]\n",
    "trg_words = [be.index2letter(y, with_start_end=False) for y in train_Y[:8000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yury/anaconda3/lib/python3.5/site-packages/nltk/translate/bleu_score.py:472: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/home/yury/anaconda3/lib/python3.5/site-packages/nltk/translate/bleu_score.py:472: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/home/yury/anaconda3/lib/python3.5/site-packages/nltk/translate/bleu_score.py:472: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7993138246330833"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib.metrics import compute_bleu_score\n",
    "\n",
    "compute_bleu_score(model, val_src_words, val_trg_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'src_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-30b43d3636f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mru_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbe_word\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mru_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mru_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_start_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbe_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'src_words' is not defined"
     ]
    }
   ],
   "source": [
    "for ru_word, be_word in zip(src_words, trg_words):\n",
    "    print(ru_word, model.translate(ru_word, with_start_end=False), be_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lib.models import BiLSTMDiscriminator\n",
    "\n",
    "disc = BiLSTMDiscriminator(be, 32, 128)\n",
    "disc_opt = optim.Adam(disc.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3864200115203857\n",
      "1.3856408596038818\n",
      "1.3859609365463257\n",
      "1.3855679035186768\n",
      "1.3860799074172974\n",
      "1.3875696659088135\n",
      "1.3844671249389648\n",
      "1.3863332271575928\n",
      "1.385830044746399\n",
      "1.3852546215057373\n",
      "1.386328935623169\n",
      "1.3852815628051758\n",
      "1.3844716548919678\n",
      "1.385976791381836\n",
      "1.3853299617767334\n",
      "1.3839329481124878\n",
      "1.3843560218811035\n",
      "1.3858451843261719\n",
      "1.3852136135101318\n",
      "1.3830749988555908\n",
      "1.382631778717041\n",
      "1.3860793113708496\n",
      "1.3807296752929688\n",
      "1.3858046531677246\n",
      "1.3817212581634521\n",
      "1.3851286172866821\n",
      "1.3851951360702515\n",
      "1.3835530281066895\n",
      "1.3865728378295898\n",
      "1.3857605457305908\n",
      "1.3793292045593262\n",
      "1.3826868534088135\n",
      "1.3832042217254639\n",
      "1.3856315612792969\n",
      "1.385282039642334\n",
      "1.3836541175842285\n",
      "1.3857414722442627\n",
      "1.3847401142120361\n",
      "1.3891725540161133\n",
      "1.385475754737854\n",
      "1.3864716291427612\n",
      "1.38339364528656\n",
      "1.3872170448303223\n",
      "1.3846826553344727\n",
      "1.3861864805221558\n",
      "1.381763219833374\n",
      "1.382333755493164\n",
      "1.3850464820861816\n",
      "1.386406660079956\n",
      "1.3868886232376099\n",
      "1.3869279623031616\n",
      "1.3828113079071045\n",
      "1.384963035583496\n",
      "1.3828485012054443\n",
      "1.3845913410186768\n",
      "1.3849701881408691\n",
      "1.3835489749908447\n",
      "1.3847448825836182\n",
      "1.3860894441604614\n",
      "1.384418249130249\n",
      "1.3869407176971436\n",
      "1.383133053779602\n",
      "1.3846724033355713\n",
      "1.3843497037887573\n",
      "1.3862004280090332\n",
      "1.3858156204223633\n",
      "1.3804774284362793\n",
      "1.384701132774353\n",
      "1.386786937713623\n",
      "1.3853248357772827\n",
      "1.3874647617340088\n",
      "1.3863976001739502\n",
      "1.3841303586959839\n",
      "1.3848518133163452\n",
      "1.3837618827819824\n",
      "1.3859648704528809\n",
      "1.3807823657989502\n",
      "1.3802158832550049\n",
      "1.3860204219818115\n",
      "1.390845537185669\n",
      "1.3855419158935547\n",
      "1.3815677165985107\n",
      "1.3720862865447998\n",
      "1.380387783050537\n",
      "1.3851823806762695\n",
      "1.3839294910430908\n",
      "1.3772051334381104\n",
      "1.3696293830871582\n",
      "1.3898565769195557\n",
      "1.3559949398040771\n",
      "1.3689908981323242\n",
      "1.3810001611709595\n",
      "1.3695746660232544\n",
      "1.3639676570892334\n",
      "1.3430705070495605\n",
      "1.3161931037902832\n",
      "1.353072166442871\n",
      "1.3380513191223145\n",
      "1.3771874904632568\n",
      "1.3437711000442505\n",
      "1.3176127672195435\n",
      "1.2892180681228638\n",
      "1.3370031118392944\n",
      "1.3443701267242432\n",
      "1.3123862743377686\n",
      "1.3367400169372559\n",
      "1.2978428602218628\n",
      "1.3176136016845703\n",
      "1.333158016204834\n",
      "1.3280534744262695\n",
      "1.273477554321289\n",
      "1.3118816553945616\n",
      "1.3182945251464844\n",
      "1.363112449645996\n",
      "1.2118465900421143\n",
      "1.2634775638580322\n",
      "1.2597577571868896\n",
      "1.3493248224258423\n",
      "1.2770062685012817\n",
      "1.281919240951538\n",
      "1.2749617099761963\n",
      "1.2099517583847046\n",
      "1.2487046718597412\n",
      "1.231416940689087\n",
      "1.2962126731872559\n",
      "1.225142002105713\n",
      "1.2385879755020142\n",
      "1.2293124198913574\n",
      "1.2921373844146729\n",
      "1.1670129299163818\n",
      "1.311342477798462\n",
      "1.2074071168899536\n",
      "1.1324577331542969\n",
      "1.3206286430358887\n",
      "1.1728707551956177\n",
      "1.2605712413787842\n",
      "1.2563982009887695\n",
      "1.2286278009414673\n",
      "1.1637158393859863\n",
      "1.2829151153564453\n",
      "1.1965863704681396\n",
      "1.356525182723999\n",
      "1.2695096731185913\n",
      "1.185196876525879\n",
      "1.3163772821426392\n",
      "1.2083194255828857\n",
      "1.3591945171356201\n",
      "1.2296028137207031\n",
      "1.3428559303283691\n",
      "1.2396754026412964\n",
      "1.1749318838119507\n",
      "1.178896188735962\n",
      "1.2726796865463257\n",
      "1.1631207466125488\n",
      "1.1881334781646729\n",
      "1.2988033294677734\n",
      "1.1839847564697266\n",
      "1.2474058866500854\n",
      "1.3573100566864014\n",
      "1.31964111328125\n",
      "1.2478318214416504\n",
      "1.19191575050354\n",
      "1.267228364944458\n",
      "1.1749482154846191\n",
      "1.10507333278656\n",
      "1.2514947652816772\n",
      "1.3536503314971924\n",
      "1.1807007789611816\n",
      "1.2642264366149902\n",
      "1.1879842281341553\n",
      "1.2723872661590576\n",
      "1.2547539472579956\n",
      "1.2845040559768677\n",
      "1.263357400894165\n",
      "1.2147364616394043\n",
      "1.2665574550628662\n",
      "1.254156470298767\n",
      "1.2326788902282715\n",
      "1.2376601696014404\n",
      "1.24013090133667\n",
      "1.215744972229004\n",
      "1.2340874671936035\n",
      "1.2852039337158203\n",
      "1.1368820667266846\n",
      "1.2844542264938354\n",
      "1.2555058002471924\n",
      "1.2860815525054932\n",
      "1.2214627265930176\n",
      "1.219545841217041\n",
      "1.1035478115081787\n",
      "1.1099085807800293\n",
      "1.1821398735046387\n",
      "1.1947338581085205\n",
      "1.2235743999481201\n",
      "1.1362738609313965\n",
      "1.2284494638442993\n",
      "1.166930913925171\n",
      "1.2605838775634766\n",
      "1.1624038219451904\n",
      "1.181697964668274\n",
      "1.2427783012390137\n",
      "1.2747247219085693\n",
      "1.2273461818695068\n",
      "1.1042497158050537\n",
      "1.241779088973999\n",
      "1.2571439743041992\n",
      "1.2116360664367676\n",
      "1.1943511962890625\n",
      "1.2219092845916748\n",
      "1.2168625593185425\n",
      "1.2896511554718018\n",
      "1.2245712280273438\n",
      "1.1145669221878052\n",
      "1.3193936347961426\n",
      "1.211572289466858\n",
      "1.1089404821395874\n",
      "1.3482472896575928\n",
      "1.1799966096878052\n",
      "1.1549032926559448\n",
      "1.282052755355835\n",
      "1.2391724586486816\n",
      "1.160329818725586\n",
      "1.2659847736358643\n",
      "1.207305193567079\n",
      "1.2791166305541992\n",
      "1.3221768140792847\n",
      "1.2193583250045776\n",
      "1.117046594619751\n",
      "1.1770074367523193\n",
      "1.157719373703003\n",
      "1.0361583232879639\n",
      "1.2256596088409424\n",
      "1.2944254875183105\n",
      "1.2345447540283203\n",
      "1.109548568725586\n",
      "1.2531862258911133\n",
      "1.2475767135620117\n",
      "1.1894938945770264\n",
      "1.2127025127410889\n",
      "1.1946606636047363\n",
      "1.1283282041549683\n",
      "1.1907353401184082\n",
      "1.2709602117538452\n",
      "1.2577929496765137\n",
      "1.188875675201416\n",
      "1.2473697662353516\n",
      "1.2150897979736328\n",
      "1.108102560043335\n",
      "1.2053691148757935\n",
      "1.2814712524414062\n",
      "1.2697827816009521\n",
      "1.2024831771850586\n",
      "1.1807332038879395\n",
      "1.215770959854126\n",
      "1.1866590976715088\n",
      "1.364619493484497\n",
      "1.0961081981658936\n",
      "1.25982666015625\n",
      "1.2110645771026611\n",
      "1.2568230628967285\n",
      "1.229736089706421\n",
      "1.1997528076171875\n",
      "1.2722125053405762\n",
      "1.18583345413208\n",
      "1.2309010028839111\n",
      "1.127612829208374\n",
      "1.1905611753463745\n",
      "1.155531406402588\n",
      "1.3416767120361328\n",
      "1.2246394157409668\n",
      "1.1074349880218506\n",
      "1.0335901975631714\n",
      "1.1810803413391113\n",
      "1.193411946296692\n",
      "1.1178733110427856\n",
      "1.2543866634368896\n",
      "1.228860855102539\n",
      "1.2961156368255615\n",
      "1.2027106285095215\n",
      "1.163858413696289\n",
      "1.3590216636657715\n",
      "1.136989712715149\n",
      "1.2764188051223755\n",
      "1.056826114654541\n",
      "1.1360998153686523\n",
      "1.093513011932373\n",
      "1.257795810699463\n",
      "1.1105849742889404\n",
      "1.156285047531128\n",
      "1.2206816673278809\n",
      "1.258082628250122\n",
      "1.080674171447754\n",
      "1.1461589336395264\n",
      "1.2336746454238892\n",
      "1.2406032085418701\n",
      "1.2484569549560547\n",
      "1.3158626556396484\n",
      "1.2452876567840576\n",
      "1.2733912467956543\n",
      "1.2379207611083984\n",
      "1.254333257675171\n",
      "1.2874791622161865\n",
      "1.2229938507080078\n",
      "1.2692190408706665\n",
      "1.1755311489105225\n",
      "1.174386978149414\n",
      "1.2354423999786377\n",
      "1.2491196393966675\n",
      "1.2134565114974976\n",
      "1.1954177618026733\n",
      "1.2024266719818115\n",
      "1.2401046752929688\n",
      "1.205686092376709\n",
      "1.225042700767517\n",
      "1.1596784591674805\n",
      "1.190022587776184\n",
      "1.108312726020813\n",
      "1.3071091175079346\n",
      "1.2557384967803955\n",
      "1.0442478656768799\n",
      "1.3238779306411743\n",
      "1.2900370359420776\n",
      "1.2718439102172852\n",
      "1.1006648540496826\n",
      "1.010657787322998\n",
      "1.2182754278182983\n",
      "1.2788207530975342\n",
      "1.345711588859558\n",
      "1.2274361848831177\n",
      "1.2162134647369385\n",
      "1.2321679592132568\n",
      "1.188849925994873\n",
      "1.2365612983703613\n",
      "1.2340364456176758\n",
      "1.1850156784057617\n",
      "1.2051768266528429\n",
      "1.2012436389923096\n",
      "1.249526023864746\n",
      "1.2724430561065674\n",
      "1.0665132999420166\n",
      "1.0747008323669434\n",
      "1.0900766849517822\n",
      "1.3224098682403564\n",
      "1.0978186130523682\n",
      "1.1418964862823486\n",
      "1.1736516952514648\n",
      "1.2240029573440552\n",
      "1.3561651706695557\n",
      "1.2611074447631836\n",
      "1.1151076555252075\n",
      "1.1996848583221436\n",
      "1.151600956916809\n",
      "1.138209581375122\n",
      "1.221709966659546\n",
      "1.19383704662323\n",
      "1.1976323127746582\n",
      "1.1635117530822754\n",
      "1.2786701917648315\n",
      "1.2594832181930542\n",
      "1.2581984996795654\n",
      "1.1516611576080322\n",
      "1.0697290897369385\n",
      "1.273655891418457\n",
      "1.1876652240753174\n",
      "1.140994906425476\n",
      "1.3166658878326416\n",
      "1.108152985572815\n",
      "1.225252389907837\n",
      "1.2633252143859863\n",
      "1.159339427947998\n",
      "1.2616112232208252\n",
      "1.2002559900283813\n",
      "1.121463418006897\n",
      "1.175811529159546\n",
      "1.146022081375122\n",
      "1.2292969226837158\n",
      "1.2441651821136475\n",
      "1.184213399887085\n",
      "1.2501287460327148\n",
      "1.2948726415634155\n",
      "1.2058659791946411\n",
      "1.107539176940918\n",
      "1.1326103210449219\n",
      "1.2279592752456665\n",
      "1.2386071681976318\n",
      "1.2558773756027222\n",
      "1.1856179237365723\n",
      "1.2131545543670654\n",
      "1.2290637493133545\n",
      "1.3483246564865112\n",
      "1.1456753015518188\n",
      "1.1849452257156372\n",
      "1.1096422672271729\n",
      "1.2369319200515747\n",
      "0.9490995407104492\n",
      "1.2228600978851318\n",
      "1.2804129123687744\n",
      "1.2238273620605469\n",
      "1.236830711364746\n",
      "1.2613368034362793\n",
      "1.2213151454925537\n",
      "1.227569341659546\n",
      "1.2068006992340088\n",
      "1.2410656213760376\n",
      "1.1801512241363525\n",
      "1.2880628108978271\n",
      "1.1626509428024292\n",
      "1.334194302558899\n",
      "1.1052632331848145\n",
      "1.1135003566741943\n",
      "1.245823621749878\n",
      "1.2009649276733398\n",
      "1.2336421012878418\n",
      "1.183868646621704\n",
      "1.166979193687439\n",
      "1.2453458309173584\n",
      "1.1722466945648193\n",
      "1.264923334121704\n",
      "1.1645166873931885\n",
      "1.1834807395935059\n",
      "1.1429939270019531\n",
      "1.1063239574432373\n",
      "1.1555089950561523\n",
      "1.2510809898376465\n",
      "1.0560532808303833\n",
      "1.1486077308654785\n",
      "1.2641370296478271\n",
      "1.0999895334243774\n",
      "1.1673352718353271\n",
      "1.187228798866272\n",
      "1.125275731086731\n",
      "1.243847131729126\n",
      "1.2733482122421265\n",
      "1.217061996459961\n",
      "1.2899749279022217\n",
      "1.227591633796692\n",
      "1.3395745754241943\n",
      "1.184269666671753\n",
      "1.099544882774353\n",
      "1.1428638696670532\n",
      "1.1488667726516724\n",
      "1.142803430557251\n",
      "1.144234299659729\n",
      "1.144784688949585\n",
      "1.2383195161819458\n",
      "1.148494839668274\n",
      "1.3445160388946533\n",
      "1.218715518321582\n",
      "1.2516705989837646\n",
      "1.2610011100769043\n",
      "1.2730375528335571\n",
      "1.2182754278182983\n",
      "1.1205387115478516\n",
      "1.2016170024871826\n",
      "1.172619342803955\n",
      "1.1820144653320312\n",
      "1.2124851942062378\n",
      "1.1332532167434692\n",
      "1.1415138244628906\n",
      "1.2154029607772827\n",
      "1.1046698093414307\n",
      "1.1768031120300293\n",
      "1.1403619050979614\n",
      "1.1725420951843262\n",
      "1.1570408344268799\n",
      "1.132073163986206\n",
      "1.0622930526733398\n",
      "1.189184546470642\n",
      "1.2031474113464355\n",
      "1.132519245147705\n",
      "1.1658350229263306\n",
      "1.136157751083374\n",
      "1.2144948244094849\n",
      "1.252689003944397\n",
      "1.2551906108856201\n",
      "1.2000572681427002\n",
      "1.2619976997375488\n",
      "1.2040212154388428\n",
      "1.1893024444580078\n",
      "1.2391080856323242\n",
      "1.3185672760009766\n",
      "1.0424554347991943\n",
      "1.1347153186798096\n",
      "1.273740530014038\n",
      "1.1522595882415771\n",
      "1.2104291915893555\n",
      "1.1316869258880615\n",
      "1.2097610235214233\n",
      "1.1957731246948242\n",
      "1.286763310432434\n",
      "1.1510047912597656\n",
      "1.026944875717163\n",
      "1.1648125648498535\n",
      "1.112890601158142\n",
      "1.1409213542938232\n",
      "1.1849918365478516\n",
      "1.2403128147125244\n",
      "1.1851458549499512\n",
      "1.168208122253418\n",
      "1.2610487937927246\n",
      "1.0949324369430542\n",
      "1.1749420166015625\n",
      "1.2677092552185059\n",
      "1.1228618621826172\n",
      "1.1480987071990967\n",
      "1.2629408836364746\n",
      "1.2060121297836304\n",
      "1.1896884441375732\n",
      "1.2063195705413818\n",
      "1.298378586769104\n",
      "1.2027463912963867\n",
      "1.1817599534988403\n",
      "1.2375882863998413\n",
      "1.2620861530303955\n",
      "1.2558577060699463\n",
      "1.2214932441711426\n",
      "1.1838622093200684\n",
      "1.1592895984649658\n",
      "1.1437262296676636\n",
      "1.3610312938690186\n",
      "1.2946114540100098\n",
      "1.198561191558838\n",
      "1.261330246925354\n",
      "1.1745808124542236\n",
      "1.1238622665405273\n",
      "1.1240825653076172\n",
      "1.3163739442825317\n",
      "1.231042742729187\n",
      "1.258788824081421\n",
      "1.190664529800415\n",
      "1.1050500869750977\n",
      "1.118228554725647\n",
      "1.1970939636230469\n",
      "1.1525696516036987\n",
      "1.1236166954040527\n",
      "1.1586973667144775\n",
      "1.298364281654358\n",
      "1.2648847103118896\n",
      "1.2165805101394653\n",
      "1.1715540885925293\n",
      "1.170357584953308\n",
      "1.2700273990631104\n",
      "1.3021228313446045\n",
      "1.1089333295822144\n",
      "1.1815907955169678\n",
      "1.2629859447479248\n",
      "1.1173150539398193\n",
      "1.2878412008285522\n",
      "1.233603835105896\n",
      "1.1537392139434814\n",
      "1.1887376308441162\n",
      "1.2621052265167236\n",
      "1.1526715755462646\n",
      "1.1633248329162598\n",
      "1.0681712627410889\n",
      "1.0353279113769531\n",
      "1.0923715829849243\n",
      "1.268265962600708\n",
      "1.2688180208206177\n",
      "1.2372437799088873\n"
     ]
    }
   ],
   "source": [
    "from lib.trainer import train_discriminator\n",
    "\n",
    "\n",
    "train_discriminator(disc, model, disc_opt, train_X, train_Y, n_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "0d3aea6021fb462089c9084c13aea847": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "223f5f5d338b400ca14ecd6cbfabcc08": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "36774b97b2a643099a1e7393e9637276": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "530775ce190b4bc189a504cb4d18d9e4": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "5de6a4101e1742b4bcc7a925a2caeee6": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "7339be0151324443a061515c1c1ad6f6": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "8568357faa8f426cb6fb722c4185f66e": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "8a67ef1a9ef14fc29cd5a91567f6b0cb": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "8ea89eee942841deadfb263c89166abc": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "8ebb018c77f04fb89c9be616ad5e55da": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "a4105ae0c7dc422fb0b3f8d5e3c99177": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "a5b1df2736324b859e0e0ca3304b1ceb": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "ae74995d441a4f5a810d0107fc58a819": {
     "views": [
      {
       "cell_index": 13
      }
     ]
    },
    "d98825d51c944998a19e4707be967940": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "ef54b3fc89d04689a6ac8933e5c8cf94": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
