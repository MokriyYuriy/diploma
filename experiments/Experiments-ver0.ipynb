{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_FILE = 'data/ru-be-train.txt'\n",
    "TEST_FILE = 'data/ru-be-test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 30\n",
    " \n",
    "class Alphabet:\n",
    "    START = '__START__'\n",
    "    END = '_END_'\n",
    " \n",
    "    def __init__(self, max_length=MAX_LENGTH):\n",
    "        \"\"\"Initialize the class which works with letter and index representations of sequences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        max_length : int\n",
    "            The largest permitted length for sequence. Longer sequences are cropped.\n",
    "        \"\"\"\n",
    "        self.max_length = max_length\n",
    "        self.letter2index_ = {Alphabet.START : 0, Alphabet.END : 1}\n",
    "        self.index2letter_ = [Alphabet.START, Alphabet.END]\n",
    "        \n",
    "    def get_index(self, letter):\n",
    "        if letter not in self.letter2index_:\n",
    "            self.letter2index_[letter] = len(self.index2letter_)\n",
    "            self.index2letter_.append(letter)\n",
    "        return self.letter2index_[letter]\n",
    "    \n",
    "    @property\n",
    "    def start_index(self):\n",
    "        return self.letter2index_[Alphabet.START]\n",
    "    \n",
    "    @property\n",
    "    def end_index(self):\n",
    "        return self.letter2index_[Alphabet.END]\n",
    "    \n",
    "    def index2letter(self, x):\n",
    "        return ''.join(self.index2letter_[index] for index in x)\n",
    "    \n",
    "    def letter2index(self, word):\n",
    "        lst = [self.get_index(letter) for letter in word]\n",
    "        return lst[:self.max_length - 1] + [self.get_index(Alphabet.END)] * max(1, self.max_length - len(lst))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.index2letter_)\n",
    "    \n",
    "    # torch utils\n",
    "    def get_length(self, input_sequence):\n",
    "        \"\"\"Infers the lengths of sequences in batch\n",
    "        \n",
    "        \"\"\"\n",
    "        return (input_sequence == self.end_index).max(dim=1)[1] + 1\n",
    "    \n",
    "    def get_mask(self, input_sequence):\n",
    "        return (torch.cumsum(input_sequence == self.end_index, dim=1) < 2).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ru = Alphabet()\n",
    "be = Alphabet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_pair_dataset(filename, alph1, alph2):\n",
    "    x, y = [], []\n",
    "    with open(filename, 'r') as ftr:\n",
    "        for line in ftr:\n",
    "            try:\n",
    "                word1, word2 = line.split()\n",
    "            except ValueError:\n",
    "                continue\n",
    "            x.append(alph1.letter2index(word1))\n",
    "            y.append(alph2.letter2index(word2))\n",
    "    return np.array(x), np.array(y)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X, train_Y = load_pair_dataset(TRAIN_FILE, ru, be)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiplicativeAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiplicativeAttention, self).__init__()\n",
    "        \n",
    "    def forward(self, decoder_hidden, encoder_hiddens):\n",
    "        weights = torch.bmm(encoder_hiddens, decoder_hidden.view(*decoder_hidden.shape, 1))\n",
    "        weights = F.softmax(Variable(weights), dim=1)\n",
    "        return torch.bmm(encoder_hiddens.transpose(1, 2), weights.data)[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attention = MultiplicativeAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoder_hidden = torch.arange(6).view(3, 2)\n",
    "encoder_hiddens = torch.arange(12).view(3, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention(decoder_hidden, encoder_hiddens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SimpleGRUEncoder(nn.Module):\n",
    "    def __init__(self, alphabet, embedding_size, hidden_size):\n",
    "        super(SimpleGRUEncoder, self).__init__()\n",
    "        self.alphabet = alphabet\n",
    "        self.embedding = nn.Embedding(num_embeddings=len(self.alphabet), embedding_dim=embedding_size)\n",
    "        self.gru = nn.GRU(input_size=embedding_size, hidden_size=hidden_size, batch_first=True)\n",
    "        \n",
    "    def forward(self, input_sequence):\n",
    "        batch_size = input_sequence.size(0)\n",
    "        embeddings = self.embedding(input_sequence)\n",
    "        out, _ = self.gru(embeddings)\n",
    "        return out[range(batch_size), self.alphabet.get_length(input_sequence) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleGRUDecoder(nn.Module):\n",
    "    def __init__(self, alphabet, embedding_size, hidden_size):\n",
    "        super(SimpleGRUDecoder, self).__init__()\n",
    "        self.alphabet = alphabet\n",
    "        self.embedding = nn.Embedding(num_embeddings=len(alphabet), embedding_dim=embedding_size)\n",
    "        self.gru_cell = nn.GRUCell(input_size=embedding_size, hidden_size=hidden_size)\n",
    "        self.logit_linear = nn.Linear(hidden_size, len(alphabet))\n",
    "        \n",
    "    def forward(self, token, prev_h):\n",
    "        embedding = self.embedding(token)\n",
    "        h = self.gru_cell(embedding, prev_h)\n",
    "        out = self.logit_linear(h)\n",
    "        return out, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SimpleGRUSupervisedSeq2Seq(nn.Module):\n",
    "    def __init__(self, src_alphabet, dst_alphabet, embedding_size, hidden_size):\n",
    "        super(SimpleGRUSupervisedSeq2Seq, self).__init__()\n",
    "        self.encoder = SimpleGRUEncoder(src_alphabet, embedding_size, hidden_size)\n",
    "        self.h_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.decoder = SimpleGRUDecoder(dst_alphabet, embedding_size, hidden_size)\n",
    "        \n",
    "    def start(self, batch_size):\n",
    "        return Variable(torch.from_numpy(np.repeat(self.decoder.alphabet.start_index, batch_size)))\n",
    "    \n",
    "    def forward(self, input_sequence, output_sequence):\n",
    "        enc_h = self.encoder(input_sequence)\n",
    "        dec_h = F.tanh(self.h_linear(enc_h))\n",
    "        logits = []\n",
    "        for x in itertools.chain((self.start(output_sequence.size(0)),), output_sequence.transpose(0, 1)[:-1]):\n",
    "            out, dec_h = self.decoder(x, dec_h)\n",
    "            logits.append(out)\n",
    "        return F.log_softmax(torch.stack(logits, dim=1), dim=-1)\n",
    "    \n",
    "    def translate(self, word, strategy='', max_length=30):\n",
    "        self.eval()\n",
    "        input_sequence = Variable(torch.from_numpy(np.array([self.encoder.alphabet.letter2index(word)])))\n",
    "        #print(input_sequence.shape)\n",
    "        hidden = F.tanh(self.h_linear(self.encoder(input_sequence)))\n",
    "        token = self.start(1)\n",
    "        #print(token.shape, hidden.shape)\n",
    "        lst = []\n",
    "        for i in range(10):\n",
    "            out, hidden = self.decoder(hidden, hidden)\n",
    "            token = out.max(1)[1]\n",
    "            #print(token, out)\n",
    "            lst.append(token.data[0])\n",
    "            if token.data[0] == self.decoder.alphabet.end_index:\n",
    "                break\n",
    "        return ''.join(self.decoder.alphabet.index2letter(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iterator(X, Y=None, batch_size=32):\n",
    "    assert Y is None or X.shape[0] == Y.shape[0]\n",
    "    ind = np.arange(X.shape[0])\n",
    "    np.random.shuffle(ind)\n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        if Y is not None:\n",
    "            yield X[ind[i:i + batch_size]], Y[ind[i:i + batch_size]]\n",
    "        else:\n",
    "            yield X[ind[i:i + batch_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SimpleGRUSupervisedSeq2Seq(ru, be, 24, 256)\n",
    "opt = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.4190867424011231\n",
      "50 2.9168739572860716\n",
      "100 2.631720038443434\n",
      "150 2.4751932120122664\n",
      "200 2.3549136442284455\n",
      "250 2.2292583443003786\n",
      "300 2.153186160866823\n",
      "350 2.0728946457414272\n",
      "400 2.007772875134808\n",
      "450 1.8948324387015085\n",
      "500 1.8464238839206966\n",
      "550 1.7897904365669128\n",
      "600 1.7117181047641865\n",
      "650 1.682086088904152\n",
      "700 1.596041369400188\n",
      "750 1.5873877769618059\n",
      "800 1.532857735104436\n",
      "850 1.4270432315501707\n",
      "900 1.3974845777059077\n",
      "950 1.394875553452432\n",
      "1000 1.3855915241216377\n",
      "1050 1.2881448232991752\n",
      "1100 1.231422457554406\n",
      "1150 1.282550341639471\n",
      "1200 1.1620309995468996\n",
      "0 1.2193324077870857\n",
      "50 1.1964899299263552\n",
      "100 1.1416526493590347\n",
      "150 1.1427525722214957\n",
      "200 1.1290796344589427\n",
      "250 1.1790902400506809\n",
      "300 1.06338651959006\n",
      "350 1.083659558269009\n",
      "400 1.0299134590515389\n",
      "450 1.0181521814348087\n",
      "500 1.0280448702756815\n",
      "550 0.9869868898738503\n",
      "600 0.9744745563684433\n",
      "650 0.9919636562126649\n",
      "700 1.0076098636803814\n",
      "750 1.0001313761563295\n",
      "800 0.9604946596105047\n",
      "850 0.9231747298422102\n",
      "900 0.9051967137445381\n",
      "950 0.8902329633063788\n",
      "1000 0.9025339591055139\n",
      "1050 0.8874946113089159\n",
      "1100 0.9257756544353434\n",
      "1150 0.8726588307098418\n",
      "1200 0.9392469943649226\n",
      "0 0.7971072145738308\n",
      "50 0.8317947246218156\n",
      "100 0.79920010748525\n",
      "150 0.7861288439750459\n",
      "200 0.8113273493033535\n",
      "250 0.7471756092004246\n",
      "300 0.826105861974919\n",
      "350 0.7615977253907759\n",
      "400 0.8175548124308465\n",
      "450 0.808349172289142\n",
      "500 0.7797781367580877\n",
      "550 0.7718988095110738\n",
      "600 0.7576589516128045\n",
      "650 0.7031085589875469\n",
      "700 0.757049078689517\n",
      "750 0.753524378061517\n",
      "800 0.7480851003537894\n",
      "850 0.7466134650902553\n",
      "900 0.7359073284772052\n",
      "950 0.7569935548038411\n",
      "1000 0.667862170844949\n",
      "1050 0.7207526713071504\n",
      "1100 0.7567298094625577\n",
      "1150 0.7095850888770208\n",
      "1200 0.7297141230758986\n",
      "0 0.6786187967832431\n",
      "50 0.6204182446500299\n",
      "100 0.5838233133133224\n",
      "150 0.6229602777319702\n",
      "200 0.6138935733478459\n",
      "250 0.6079781332661911\n",
      "300 0.6059271560271886\n",
      "350 0.5773672819503219\n",
      "400 0.6393599081797505\n",
      "450 0.6120285677319225\n",
      "500 0.6414565558008618\n",
      "550 0.5828210491555962\n",
      "600 0.59139657256988\n",
      "650 0.5421433046442072\n",
      "700 0.6266427414586868\n",
      "750 0.5806260052707036\n",
      "800 0.5998942692771593\n",
      "850 0.5911448041674844\n",
      "900 0.5861805673962391\n",
      "950 0.5609365153166396\n",
      "1000 0.5656277103988533\n",
      "1050 0.5990720687850405\n",
      "1100 0.577634195581245\n",
      "1150 0.6164810681449383\n",
      "1200 0.605216256388302\n",
      "0 0.607536234952715\n",
      "50 0.492946371108137\n",
      "100 0.48226420224854144\n",
      "150 0.45669573418775034\n",
      "200 0.529136255274264\n",
      "250 0.4607847133626608\n",
      "300 0.5062779492579758\n",
      "350 0.5450863855211812\n",
      "400 0.5080255382846917\n",
      "450 0.49897483158234684\n",
      "500 0.49285365006922593\n",
      "550 0.518295401711792\n",
      "600 0.48089864123436543\n",
      "650 0.502335007305534\n",
      "700 0.5153676600910416\n",
      "750 0.5163773896773239\n",
      "800 0.45892089994614016\n",
      "850 0.4457330263962269\n",
      "900 0.5190317325614101\n",
      "950 0.4813676268684634\n",
      "1000 0.4741026914101667\n",
      "1050 0.538057705110213\n",
      "1100 0.4602227714498587\n",
      "1150 0.4688909207479882\n",
      "1200 0.5136660039579245\n",
      "0 0.4903953831146079\n",
      "50 0.4464324071855099\n",
      "100 0.42509340250399824\n",
      "150 0.4258445725953515\n",
      "200 0.4286734643815699\n",
      "250 0.3935452231927976\n",
      "300 0.3557918167169828\n",
      "350 0.41821491685128737\n",
      "400 0.45435394252285044\n",
      "450 0.43394496570350327\n",
      "500 0.3922531094776658\n",
      "550 0.4225691594436006\n",
      "600 0.3844889277495822\n",
      "650 0.4139183837795619\n",
      "700 0.4363383409848634\n",
      "750 0.48793013050434053\n",
      "800 0.3992628590858384\n",
      "850 0.41539320165486304\n",
      "900 0.4352877360284689\n",
      "950 0.43099293919239\n",
      "1000 0.4055528422348902\n",
      "1050 0.39506881931438353\n",
      "1100 0.42461640479064605\n",
      "1150 0.45173071162276157\n",
      "1200 0.4215302849662691\n",
      "0 0.44703985098638843\n",
      "50 0.35997746643448847\n",
      "100 0.3331484204483485\n",
      "150 0.345207065487615\n",
      "200 0.3758729802654624\n",
      "250 0.32824332496672437\n",
      "300 0.32967295938189295\n",
      "350 0.34791981520869514\n",
      "400 0.3443642458312539\n",
      "450 0.3764324612294274\n",
      "500 0.33257793953129117\n",
      "550 0.3630270097324052\n",
      "600 0.368511933002805\n",
      "650 0.354349679247596\n",
      "700 0.3539308855836114\n",
      "750 0.35115133185365605\n",
      "800 0.3437728217407081\n",
      "850 0.3504672108467991\n",
      "900 0.3826659834888892\n",
      "950 0.3454312227033519\n",
      "1000 0.40594612139741326\n",
      "1050 0.37855045452290265\n",
      "1100 0.4116489141882962\n",
      "1150 0.3631634044302275\n",
      "1200 0.397604347298777\n",
      "0 0.34881253203525586\n",
      "50 0.29546929063416427\n",
      "100 0.2870783111675409\n",
      "150 0.2966577867288727\n",
      "200 0.2768290089832446\n",
      "250 0.28052223277822325\n",
      "300 0.3155259264706172\n",
      "350 0.3166861595198928\n",
      "400 0.2825127018072251\n",
      "450 0.2958356271954168\n",
      "500 0.2707944887245264\n",
      "550 0.31138470658314377\n",
      "600 0.30791771244184923\n",
      "650 0.2981332514500259\n",
      "700 0.30443989129391164\n",
      "750 0.2874250298341102\n",
      "800 0.3291903733968144\n",
      "850 0.3565565527882422\n",
      "900 0.3162007242302764\n",
      "950 0.32467646550824836\n",
      "1000 0.32278234944146655\n",
      "1050 0.34319290080907666\n",
      "1100 0.3482693689697987\n",
      "1150 0.3399977181636419\n",
      "1200 0.3667521389831094\n",
      "0 0.320228697751113\n",
      "50 0.25469180285812487\n",
      "100 0.2383662506199291\n",
      "150 0.256120338101611\n",
      "200 0.24049277143658915\n",
      "250 0.24746424747483764\n",
      "300 0.2739380622040691\n",
      "350 0.2771277026074438\n",
      "400 0.2516287557705043\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-e2f19b74f12c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mlog_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;31m#print(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yury/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-2ad206dd9c69>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_sequence, output_sequence)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yury/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-ca0f20893a2f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, token, prev_h)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogit_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yury/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yury/anaconda3/lib/python3.5/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_hh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_hh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m         )\n",
      "\u001b[0;32m/home/yury/anaconda3/lib/python3.5/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mGRUCell\u001b[0;34m(input, hidden, w_ih, w_hh, b_ih, b_hh)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mresetgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_r\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0minputgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_i\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mnewgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_n\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresetgate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mh_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mhy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewgate\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minputgate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnewgate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yury/anaconda3/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mtanh\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    805\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTanh\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \"\"\"\n\u001b[0;32m--> 807\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def cross_entropy(log_predictions, targets, alphabet):\n",
    "    \"\"\" Cross entropy loss for sequences\n",
    "    Parameters\n",
    "    ---------\n",
    "    log_predictions: Tensor NxTxH\n",
    "        Log probabilities\n",
    "    targets: Tensor NxT\n",
    "        True index-encoded translations\n",
    "    alphabet: Alphabet\n",
    "        Alphabet object\n",
    "    \n",
    "    \"\"\"\n",
    "    length_mask = alphabet.get_mask(targets)\n",
    "    targets_mask = torch.zeros_like(log_predictions).scatter_(2, targets.view(*targets.shape, 1), 1.0)\n",
    "    mask = targets_mask * length_mask.view(*length_mask.shape, 1)\n",
    "    #print(mask.sum(1, keepdim=True).sum(2, keepdim=True))\n",
    "    return (log_predictions * mask / (mask.sum(2, keepdim=True).sum(1, keepdim=True) * -log_predictions.size(0))).sum()\n",
    " \n",
    "cur_loss = 0\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    for i, (x, y) in enumerate(batch_iterator(train_X, train_Y)):\n",
    "        inputs = Variable(torch.from_numpy(x))\n",
    "        targets = Variable(torch.from_numpy(y))\n",
    "        log_predictions = model(inputs, targets)\n",
    "        #print(x)\n",
    "        loss = cross_entropy(log_predictions, targets, be)\n",
    "        #print(loss.data, log_predictions.data.min())\n",
    "        loss.backward()\n",
    "        cur_loss = 0.9 * cur_loss + 0.1 * loss.data[0]\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        if i % 50 == 0:\n",
    "            print(i, cur_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def translate(model, word, strategy='', max_length=30):\n",
    "    model.eval()\n",
    "    input_sequence = Variable(torch.from_numpy(np.array([model.encoder.alphabet.letter2index(word)])))\n",
    "    #print(input_sequence.shape)\n",
    "    hidden = F.tanh(model.h_linear(model.encoder(input_sequence)))\n",
    "    token = model.start(1)\n",
    "    #print(token.shape, hidden.shape)\n",
    "    lst = []\n",
    "    for i in range(max_length):\n",
    "        out, hidden = model.decoder(token, hidden)\n",
    "        token = out.max(1)[1]\n",
    "        #print(token, out)\n",
    "        lst.append(token.data[0])\n",
    "        if token.data[0] == model.decoder.alphabet.end_index:\n",
    "            break\n",
    "    return ''.join(model.decoder.alphabet.index2letter(lst))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_END_'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import editdistance as ed\n",
    "\n",
    "\n",
    "scs = []\n",
    "with open(TEST_FILE, \"r\") as ftr:\n",
    "    for ruw, bew in map(lambda x: x.split(), \n",
    "                        filter(lambda x: len(x.split()) == 2, tqdm_notebook(ftr.readlines()))):\n",
    "        res = translate(model, ruw)\n",
    "        scs.append(ed.eval(bew, res[:-5]))\n",
    "        #print(ruw, bew, res)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9062119983785974"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "поражением\tпаразай\r\n",
      "местному\tмясцовым\r\n",
      "испанских\tіспанскіх\r\n",
      "сорока\tсарока\r\n",
      "способна\tздольная\r\n",
      "факторы\tфактары\r\n",
      "высадки\tвысадкі\r\n",
      "аргентина\tаргенціна\r\n",
      "феликс\tфелікс\r\n",
      "финальном\tфінальным\r\n",
      "фактором\tфактарам\r\n",
      "оригинального\tарыгінальнага\r\n",
      "древнейших\tнайстаражытных\r\n",
      "художественным\tмастацкім\r\n",
      "резиденции\tрэзідэнцыі\r\n",
      "кораблях\tкараблях\r\n",
      "православие\tправаслаўе\r\n",
      "мл\tмл\r\n",
      "гражданский\tграмадзянскі\r\n",
      "транспортного\tтранспартнага\r\n",
      "листьями\tлісцем\r\n",
      "поврежд\tповрежд\r\n",
      "выехал\tвыехаў\r\n",
      "сицилии\tсіцыліі\r\n",
      "чтение\tчытанне\r\n",
      "повсеместно\tпаўсюдна\r\n",
      "украшения\tўпрыгажэнні\r\n",
      "передали\tперадалі\r\n",
      "проявления\tпраявы\r\n",
      "вокалист\tвакаліст\r\n",
      "склад\tсклад\r\n",
      "уголовное\tкрымінальная\r\n",
      "работало\tпрацавала\r\n",
      "ушел\tсышоў\r\n",
      "запросов\tзапытаў\r\n",
      "спектакля\tспектакля\r\n",
      "продать\tпрадаць\r\n",
      "сл\tсл\r\n",
      "ленинского\tленінскага\r\n",
      "землями\tземлямі\r\n",
      "свидетелей\tсведак\r\n",
      "существующей\tіснуючай\r\n",
      "частного\tпрыватнага\r\n",
      "образцу\tузоры\r\n",
      "отмечали\tадзначалі\r\n",
      "воздействием\tуздзеяннем\r\n",
      "категорически\tкатэгарычна\r\n",
      "дальнейшие\tдалейшыя\r\n",
      "сообщается\tпаведамляецца\r\n",
      "периоде\tперыядзе\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 50 data/ru-be.txt"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "70a85114205046dcb25e3de08fb5775c": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
