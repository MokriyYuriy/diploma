{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading https://files.pythonhosted.org/packages/e8/c5/0763a145e051ce7c84c128621693d1c5dfad5a42d551e8d79742261002e2/torch-0.3.1-cp35-cp35m-manylinux1_x86_64.whl (496.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 496.4MB 2.3kB/s eta 0:00:01  7% |██▌                             | 38.6MB 35.4MB/s eta 0:00:13    10% |███▍                            | 52.1MB 39.0MB/s eta 0:00:12    13% |████▍                           | 68.3MB 26.1MB/s eta 0:00:17    14% |████▊                           | 73.2MB 26.8MB/s eta 0:00:16    15% |█████                           | 76.5MB 37.9MB/s eta 0:00:12    19% |██████▏                         | 95.2MB 40.8MB/s eta 0:00:10    20% |██████▍                         | 99.5MB 33.0MB/s eta 0:00:13    24% |████████                        | 122.5MB 21.3MB/s eta 0:00:18    27% |████████▊                       | 135.9MB 42.2MB/s eta 0:00:09    31% |██████████                      | 155.7MB 44.5MB/s eta 0:00:08    33% |██████████▉                     | 168.7MB 48.6MB/s eta 0:00:07    36% |███████████▋                    | 179.6MB 45.6MB/s eta 0:00:07    43% |██████████████                  | 215.7MB 37.5MB/s eta 0:00:08    43% |██████████████                  | 218.0MB 40.0MB/s eta 0:00:07    46% |███████████████                 | 231.5MB 25.5MB/s eta 0:00:11    52% |████████████████▉               | 261.7MB 44.2MB/s eta 0:00:06    58% |██████████████████▊             | 289.8MB 42.8MB/s eta 0:00:05    62% |████████████████████▏           | 312.1MB 37.5MB/s eta 0:00:05    72% |███████████████████████▎        | 360.4MB 40.9MB/s eta 0:00:04    74% |████████████████████████        | 372.3MB 36.0MB/s eta 0:00:04    75% |████████████████████████▎       | 376.3MB 34.8MB/s eta 0:00:04    76% |████████████████████████▍       | 378.3MB 36.9MB/s eta 0:00:04    77% |█████████████████████████       | 386.6MB 49.8MB/s eta 0:00:03B 48.1MB/s eta 0:00:03�█████▏     | 405.9MB 42.9MB/s eta 0:00:03��████████▋     | 412.0MB 42.8MB/s eta 0:00:02��█████████     | 417.5MB 45.8MB/s eta 0:00:02�████████████████▌    | 426.9MB 42.2MB/s eta 0:00:028% |████████████████████████████▏   | 437.4MB 26.4MB/s eta 0:00:03██████████████████████████▏   | 437.7MB 28.0MB/s eta 0:00:03��██████████████████████▎   | 438.0MB 30.3MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/c9/f4eb36734bffd36eb8095247d816cbe6aeca0a2b9218b78678288edfdb92/torchvision-0.2.0-py2.py3-none-any.whl (48kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 2.6MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting pyyaml (from torch)\n",
      "  Downloading https://files.pythonhosted.org/packages/4a/85/db5a2df477072b2902b0eb892feb37d88ac635d36245a72a6a69b23b383a/PyYAML-3.12.tar.gz (253kB)\n",
      "\u001b[K    100% |████████████████████████████████| 256kB 3.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy (from torch)\n",
      "  Downloading https://files.pythonhosted.org/packages/ea/31/991207e6234b46a1228be970735ead9d6f06a298917d6f718c5e32e835bb/numpy-1.14.2-cp35-cp35m-manylinux1_x86_64.whl (12.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 12.1MB 104kB/s eta 0:00:01    90% |████████████████████████████▉   | 10.9MB 35.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six (from torchvision)\n",
      "  Downloading https://files.pythonhosted.org/packages/67/4b/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a/six-1.11.0-py2.py3-none-any.whl\n",
      "Collecting pillow>=4.1.1 (from torchvision)\n",
      "  Downloading https://files.pythonhosted.org/packages/07/52/8e27b9c54cb70d379244771a58483928b3a02db3c657d466ed84eb18f22b/Pillow-5.1.0-cp35-cp35m-manylinux1_x86_64.whl (2.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.0MB 593kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyyaml\n",
      "  Running setup.py bdist_wheel for pyyaml ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/mavrandr/.cache/pip/wheels/03/05/65/bdc14f2c6e09e82ae3e0f13d021e1b6b2481437ea2f207df3f\n",
      "Successfully built pyyaml\n",
      "Installing collected packages: pyyaml, numpy, torch, six, pillow, torchvision\n",
      "Successfully installed numpy-1.14.2 pillow-5.1.0 pyyaml-3.12 six-1.11.0 torch-0.3.1 torchvision-0.2.0\n",
      "\u001b[33mYou are using pip version 8.1.1, however version 9.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install --user torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_FILE = 'data/ru-be-train.txt'\n",
    "TEST_FILE = 'data/ru-be-test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 30\n",
    " \n",
    "class Alphabet:\n",
    "    START = '__START__'\n",
    "    END = '_END_'\n",
    " \n",
    "    def __init__(self, max_length=MAX_LENGTH):\n",
    "        \"\"\"Initialize the class which works with letter and index representations of sequences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        max_length : int\n",
    "            The largest permitted length for sequence. Longer sequences are cropped.\n",
    "        \"\"\"\n",
    "        self.max_length = max_length\n",
    "        self.letter2index_ = {Alphabet.START : 0, Alphabet.END : 1}\n",
    "        self.index2letter_ = [Alphabet.START, Alphabet.END]\n",
    "        \n",
    "    def get_index(self, letter):\n",
    "        if letter not in self.letter2index_:\n",
    "            self.letter2index_[letter] = len(self.index2letter_)\n",
    "            self.index2letter_.append(letter)\n",
    "        return self.letter2index_[letter]\n",
    "    \n",
    "    @property\n",
    "    def start_index(self):\n",
    "        return self.letter2index_[Alphabet.START]\n",
    "    \n",
    "    @property\n",
    "    def end_index(self):\n",
    "        return self.letter2index_[Alphabet.END]\n",
    "    \n",
    "    def index2letter(self, x):\n",
    "        result = []\n",
    "        for index in x:\n",
    "            result.append(self.index2letter_[index])\n",
    "            if index == self.end_index:\n",
    "                break\n",
    "        return ''.join(result)\n",
    "    \n",
    "    def letter2index(self, word):\n",
    "        lst = [self.get_index(letter) for letter in word]\n",
    "        return lst[:self.max_length - 1] + [self.get_index(Alphabet.END)] * max(1, self.max_length - len(lst))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.index2letter_)\n",
    "    \n",
    "    # torch utils\n",
    "    def get_length(self, input_sequence):\n",
    "        \"\"\"Infers the lengths of the sequences in batch\n",
    "        \n",
    "        input_sequence: Tensor NxT\n",
    "        \n",
    "        returs: Tensor N\n",
    "        \"\"\"\n",
    "        return (input_sequence == self.end_index).max(dim=1)[1] + 1\n",
    "    \n",
    "    def get_mask(self, input_sequence):\n",
    "        \"\"\"Infers the mask of the sequences in batch\n",
    "        \n",
    "        input_sequence: Tensor NxT\n",
    "        \n",
    "        returns: Tensor NxT contained 0s and 1s.\n",
    "        \"\"\"\n",
    "        return (torch.cumsum(input_sequence == self.end_index, dim=1) < 2).float()\n",
    "    \n",
    "    def get_one_hot_repr(self, input_sequence):\n",
    "        \"\"\"Produces one_hot representation from label representation/\n",
    "        \n",
    "        input_sequence: LongTensor NxT\n",
    "        \n",
    "        returns: FloatTensor NxTxH\n",
    "        \"\"\"\n",
    "        \n",
    "        onehot = torch.FloatTensor(*input_sequence.shape, len(self)).zero_()\n",
    "        onehot.scatter_(2, input_sequence.unsqueeze(2), 1.)\n",
    "        \n",
    "        return onehot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ru = Alphabet()\n",
    "be = Alphabet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_pair_dataset(filename, alph1, alph2):\n",
    "    x, y = [], []\n",
    "    with open(filename, 'r') as ftr:\n",
    "        for line in ftr:\n",
    "            try:\n",
    "                word1, word2 = line.split()\n",
    "            except ValueError:\n",
    "                continue\n",
    "            x.append(alph1.letter2index(word1))\n",
    "            y.append(alph2.letter2index(word2))\n",
    "    return np.array(x), np.array(y)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y = load_pair_dataset(TRAIN_FILE, ru, be)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "onehot = torch.FloatTensor(2, 2, 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indx = torch.LongTensor([[1, 4], [4, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "onehot.scatter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, val_X, train_Y, val_Y = train_test_split(X, Y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_softmax_with_mask(entries, mask):\n",
    "    entries = entries[:,:,0]\n",
    "    maxs = entries.max(1, keepdim=True)[0]\n",
    "    #print(entries.shape, maxs.shape, mask.shape)\n",
    "    entries = torch.exp(entries - maxs) * mask\n",
    "    return entries / (entries.sum(dim=1, keepdim=True) + 1e-15)\n",
    "\n",
    "\n",
    "class MultiplicativeAttentionWithMask(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MultiplicativeAttentionWithMask, self).__init__()\n",
    "        self.encoder_linear = nn.Linear(input_size, output_size)\n",
    "        self.decoder_linear = nn.Linear(input_size, output_size)\n",
    "        \n",
    "    def forward(self, decoder_hidden, encoder_hiddens, encoder_mask):\n",
    "        \"\"\"\n",
    "        decoder_hidden: NxH\n",
    "        encoder_hiddens: NxTxH\n",
    "        \"\"\"\n",
    "        decoder_hidden_key = F.tanh(self.decoder_linear(decoder_hidden))\n",
    "        encoder_hiddens_keys = F.tanh(self.encoder_linear(encoder_hiddens))\n",
    "        weights = torch.bmm(encoder_hiddens_keys, decoder_hidden_key.unsqueeze(2))\n",
    "        weights = seq2seq_softmax_with_mask(weights, encoder_mask)\n",
    "        return torch.bmm(encoder_hiddens.transpose(1, 2), weights.unsqueeze(2))[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleGRUEncoder(nn.Module):\n",
    "    def __init__(self, alphabet, embedding_size, hidden_size):\n",
    "        super(SimpleGRUEncoder, self).__init__()\n",
    "        self.alphabet = alphabet\n",
    "        self.embedding = nn.Embedding(num_embeddings=len(self.alphabet), embedding_dim=embedding_size)\n",
    "        self.gru = nn.GRU(input_size=embedding_size, hidden_size=hidden_size, batch_first=True)\n",
    "        \n",
    "    def forward(self, input_sequence):\n",
    "        batch_size = input_sequence.size(0)\n",
    "        embeddings = self.embedding(input_sequence)\n",
    "        out, _ = self.gru(embeddings)\n",
    "        return out, self.alphabet.get_mask(input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleGRUDecoderWithAttention(nn.Module):\n",
    "    def __init__(self, alphabet, embedding_size, hidden_size):\n",
    "        super(SimpleGRUDecoderWithAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.alphabet = alphabet\n",
    "        self.embedding = nn.Embedding(num_embeddings=len(alphabet), embedding_dim=embedding_size)\n",
    "        self.gru_cell = nn.GRUCell(input_size=embedding_size + hidden_size, hidden_size=hidden_size)\n",
    "        self.logit_linear = nn.Linear(hidden_size, len(alphabet))\n",
    "        self.attention = MultiplicativeAttentionWithMask(hidden_size, embedding_size)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        return Variable(torch.zeros(batch_size, self.hidden_size))\n",
    "        \n",
    "    def forward(self, token, prev_h, encoder_hs, encoder_mask):\n",
    "        embedding = self.embedding(token)\n",
    "        attention = self.attention(prev_h, encoder_hs, encoder_mask)\n",
    "        #print(attention.shape, embedding.shape)\n",
    "        h = self.gru_cell(torch.cat((embedding, attention), dim=1), prev_h)\n",
    "        out = self.logit_linear(h)\n",
    "        return out, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleGRUSupervisedSeq2Seq(nn.Module):\n",
    "    def __init__(self, src_alphabet, dst_alphabet, embedding_size, hidden_size):\n",
    "        super(SimpleGRUSupervisedSeq2Seq, self).__init__()\n",
    "        self.encoder = SimpleGRUEncoder(src_alphabet, embedding_size, hidden_size)\n",
    "        self.h_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.decoder = SimpleGRUDecoderWithAttention(dst_alphabet, embedding_size, hidden_size)\n",
    "        \n",
    "    def start(self, batch_size):\n",
    "        return Variable(torch.from_numpy(np.repeat(self.decoder.alphabet.start_index, batch_size)))\n",
    "    \n",
    "    '''\n",
    "    def middle_layer(self, out, mask):\n",
    "        #print(mask.sum(1))\n",
    "        return F.tanh(self.h_linear(out[range(out.shape[0]), mask.sum(1).long() - 1]))\n",
    "    '''\n",
    "    \n",
    "    def forward(self, input_sequence, output_sequence):\n",
    "        enc_out, enc_mask = self.encoder(input_sequence)\n",
    "        dec_h = self.decoder.init_hidden(input_sequence.size(0))\n",
    "        logits = []\n",
    "        for x in itertools.chain((self.start(output_sequence.size(0)),), output_sequence.transpose(0, 1)[:-1]):\n",
    "            out, dec_h = self.decoder(x, dec_h, enc_out, enc_mask)\n",
    "            logits.append(out)\n",
    "        return F.log_softmax(torch.stack(logits, dim=1), dim=-1)\n",
    "    \n",
    "    def translate(self, word, strategy='', max_length=30):\n",
    "        if isinstance(word, str):\n",
    "            input_sequence = Variable(torch.from_numpy(np.array([model.encoder.alphabet.letter2index(word)])))\n",
    "        elif isinstance(word, np.ndarray):\n",
    "            input_sequence = Variable(torch.from_numpy(word[np.newaxis]))\n",
    "        else:\n",
    "            assert False, \"word argument must be str or numpy array\"\n",
    "        #print(input_sequence.shape)\n",
    "        enc_out, enc_mask = self.encoder(input_sequence)\n",
    "        hidden = self.decoder.init_hidden(input_sequence.size(0))\n",
    "        token = self.start(1)\n",
    "        #print(token.shape, hidden.shape)\n",
    "        lst = []\n",
    "        for i in range(max_length):\n",
    "            out, hidden = self.decoder(token, hidden, enc_out, enc_mask)\n",
    "            token = out.max(1)[1]\n",
    "            #print(token, out)\n",
    "            lst.append(token.data[0])\n",
    "            if token.data[0] == self.decoder.alphabet.end_index:\n",
    "                break\n",
    "        return ''.join(model.decoder.alphabet.index2letter(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iterator(X, Y=None, batch_size=32):\n",
    "    assert Y is None or X.shape[0] == Y.shape[0]\n",
    "    ind = np.arange(X.shape[0])\n",
    "    np.random.shuffle(ind)\n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        if Y is not None:\n",
    "            yield X[ind[i:i + batch_size]], Y[ind[i:i + batch_size]]\n",
    "        else:\n",
    "            yield X[ind[i:i + batch_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SimpleGRUSupervisedSeq2Seq(ru, be, 65, 256)\n",
    "opt = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.state_dict of SimpleGRUSupervisedSeq2Seq(\n",
       "  (encoder): SimpleGRUEncoder(\n",
       "    (embedding): Embedding(35, 65)\n",
       "    (gru): GRU(65, 256, batch_first=True)\n",
       "  )\n",
       "  (h_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (decoder): SimpleGRUDecoderWithAttention(\n",
       "    (embedding): Embedding(65, 65)\n",
       "    (gru_cell): GRUCell(321, 256)\n",
       "    (logit_linear): Linear(in_features=256, out_features=65, bias=True)\n",
       "    (attention): MultiplicativeAttentionWithMask(\n",
       "      (encoder_linear): Linear(in_features=256, out_features=65, bias=True)\n",
       "      (decoder_linear): Linear(in_features=256, out_features=65, bias=True)\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy(log_predictions, targets, alphabet):\n",
    "    \"\"\" Cross entropy loss for sequences\n",
    "    Parameters\n",
    "    ---------\n",
    "    log_predictions: Tensor NxTxH\n",
    "        Log probabilities\n",
    "    targets: Tensor NxT\n",
    "        True index-encoded translations\n",
    "    alphabet: Alphabet\n",
    "        Alphabet object\n",
    "    \n",
    "    \"\"\"\n",
    "    length_mask = alphabet.get_mask(targets)\n",
    "    targets_mask = torch.zeros_like(log_predictions).scatter_(2, targets.view(*targets.shape, 1), 1.0)\n",
    "    mask = targets_mask * length_mask.view(*length_mask.shape, 1)\n",
    "    #print(mask.sum(1, keepdim=True).sum(2, keepdim=True))\n",
    "    return (log_predictions * mask / (mask.sum(2, keepdim=True).sum(1, keepdim=True) * -log_predictions.size(0))).sum()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_src_words = [ru.index2letter(x) for x in val_X]\n",
    "val_trg_words = [be.index2letter(y) for y in val_Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 iter: 49 loss: 2.658094564029786\n",
      "epoch: 0 iter: 99 loss: 1.9452701785001625\n",
      "epoch: 0 iter: 149 loss: 1.5060542984939285\n",
      "epoch: 0 iter: 199 loss: 1.2539703208550455\n",
      "epoch: 0 iter: 249 loss: 1.1064925362440883\n",
      "epoch: 0 iter: 299 loss: 1.1126237672424175\n",
      "epoch: 0 iter: 349 loss: 0.9517145791588428\n",
      "epoch: 0 iter: 399 loss: 0.9527058021153767\n",
      "epoch: 0 iter: 449 loss: 0.944212528404\n",
      "epoch: 0 iter: 499 loss: 0.9366004106275856\n",
      "epoch: 0 iter: 549 loss: 0.898973423502346\n",
      "epoch: 0 iter: 599 loss: 0.8774192931626411\n",
      "epoch: 0 iter: 649 loss: 0.8687348401024029\n",
      "epoch: 0 iter: 699 loss: 0.8806170715339428\n",
      "epoch: 0 iter: 749 loss: 0.8417171325990492\n",
      "epoch: 0 iter: 799 loss: 0.8754202066837747\n",
      "epoch: 0 iter: 949 loss: 0.7674467000063532\n",
      "epoch: 0 iter: 999 loss: 0.7958143459931212\n",
      "epoch: 0 iter: 1049 loss: 0.8259128763589467\n",
      "epoch: 0 iter: 1099 loss: 0.8078720349438748\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a5cefc7e5449dd9f2610985ff85c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3951), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mavrandr/.local/lib/python3.5/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/home/mavrandr/.local/lib/python3.5/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/home/mavrandr/.local/lib/python3.5/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 iter: 49 loss: 0.7564559385103249\n",
      "epoch: 1 iter: 99 loss: 0.6769460815427547\n",
      "epoch: 1 iter: 149 loss: 0.6797774114468229\n",
      "epoch: 1 iter: 199 loss: 0.7332826297063264\n",
      "epoch: 1 iter: 349 loss: 0.7315959323937342\n",
      "epoch: 1 iter: 399 loss: 0.6980103078134308\n",
      "epoch: 1 iter: 449 loss: 0.6954559927595563\n",
      "epoch: 1 iter: 499 loss: 0.6728843532871932\n",
      "epoch: 1 iter: 549 loss: 0.6983245302508848\n",
      "epoch: 1 iter: 599 loss: 0.6933372382867709\n",
      "epoch: 1 iter: 649 loss: 0.6586113522930109\n",
      "epoch: 1 iter: 699 loss: 0.681526105897865\n",
      "epoch: 1 iter: 749 loss: 0.664668386590138\n",
      "epoch: 1 iter: 799 loss: 0.631510930791135\n",
      "epoch: 1 iter: 849 loss: 0.6717778035628991\n",
      "epoch: 1 iter: 899 loss: 0.6294165461083708\n",
      "epoch: 1 iter: 949 loss: 0.6365922626680415\n",
      "epoch: 1 iter: 999 loss: 0.6433808709181964\n",
      "epoch: 1 iter: 1049 loss: 0.6359586440487551\n",
      "epoch: 1 iter: 1099 loss: 0.598834692402115\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a23fe995acc4aa1ae2929e04c870f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3951), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 1 val_score: 0.7112108952112097 time: 1126.3234903812408\n",
      "epoch: 2 iter: 49 loss: 0.5500012394054368\n",
      "epoch: 2 iter: 99 loss: 0.5719838355182909\n",
      "epoch: 2 iter: 149 loss: 0.6140890254027743\n",
      "epoch: 2 iter: 199 loss: 0.5894810584898776\n",
      "epoch: 2 iter: 249 loss: 0.5916134667050021\n",
      "epoch: 2 iter: 299 loss: 0.5911880229460058\n",
      "epoch: 2 iter: 349 loss: 0.590636631945699\n",
      "epoch: 2 iter: 399 loss: 0.49537212592801494\n",
      "epoch: 2 iter: 449 loss: 0.5731639161831696\n",
      "epoch: 2 iter: 499 loss: 0.5938712321965762\n",
      "epoch: 2 iter: 549 loss: 0.642468438016881\n",
      "epoch: 2 iter: 599 loss: 0.6061879933540106\n",
      "epoch: 2 iter: 649 loss: 0.5905270646446676\n",
      "epoch: 2 iter: 699 loss: 0.5457749712378999\n",
      "epoch: 2 iter: 749 loss: 0.6195805045405616\n",
      "epoch: 2 iter: 799 loss: 0.5690627256651999\n",
      "epoch: 2 iter: 849 loss: 0.5704222785395039\n",
      "epoch: 2 iter: 899 loss: 0.5782761578177854\n",
      "epoch: 2 iter: 949 loss: 0.5897927144299605\n",
      "epoch: 2 iter: 999 loss: 0.5705088882869493\n",
      "epoch: 2 iter: 1049 loss: 0.5073121535080907\n",
      "epoch: 2 iter: 1099 loss: 0.5431713649309331\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1705a17e003843598d3209413c29aebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3951), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 2 val_score: 0.7182228578595783 time: 1124.7359964847565\n",
      "epoch: 3 iter: 49 loss: 0.5347833189316005\n",
      "epoch: 3 iter: 99 loss: 0.46868262541255357\n",
      "epoch: 3 iter: 149 loss: 0.5451921168764445\n",
      "epoch: 3 iter: 199 loss: 0.5298714926583855\n",
      "epoch: 3 iter: 249 loss: 0.4974048926487826\n",
      "epoch: 3 iter: 299 loss: 0.4501444181505279\n",
      "epoch: 3 iter: 349 loss: 0.48753632086869236\n",
      "epoch: 3 iter: 399 loss: 0.4938887831833474\n",
      "epoch: 3 iter: 449 loss: 0.5047089317457525\n",
      "epoch: 3 iter: 499 loss: 0.49194954649047673\n",
      "epoch: 3 iter: 549 loss: 0.46731216346731214\n",
      "epoch: 3 iter: 599 loss: 0.461073226839438\n",
      "epoch: 3 iter: 649 loss: 0.5140152006705162\n",
      "epoch: 3 iter: 699 loss: 0.48908072286892657\n",
      "epoch: 3 iter: 749 loss: 0.4939158150918599\n",
      "epoch: 3 iter: 799 loss: 0.5115724518267698\n",
      "epoch: 3 iter: 849 loss: 0.48050468354417397\n",
      "epoch: 3 iter: 899 loss: 0.45724669590551703\n",
      "epoch: 3 iter: 949 loss: 0.4387043664267125\n",
      "epoch: 3 iter: 999 loss: 0.46182817783749497\n",
      "epoch: 3 iter: 1049 loss: 0.48179894475412166\n",
      "epoch: 3 iter: 1099 loss: 0.4799370734339935\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf82ad0a0d7a442fb1e614c5f064436c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3951), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 3 val_score: 0.7258356677385501 time: 1124.679298400879\n",
      "epoch: 4 iter: 49 loss: 0.4347603522795892\n",
      "epoch: 4 iter: 99 loss: 0.454316446111022\n",
      "epoch: 4 iter: 149 loss: 0.41556501353764386\n",
      "epoch: 4 iter: 199 loss: 0.3891953656405022\n",
      "epoch: 4 iter: 249 loss: 0.4173830206069239\n",
      "epoch: 4 iter: 299 loss: 0.42057612199415245\n",
      "epoch: 4 iter: 349 loss: 0.41305324171667346\n",
      "epoch: 4 iter: 399 loss: 0.4225158939252388\n",
      "epoch: 4 iter: 449 loss: 0.4143450293980468\n",
      "epoch: 4 iter: 499 loss: 0.3995721722425545\n",
      "epoch: 4 iter: 549 loss: 0.4456956018866122\n",
      "epoch: 4 iter: 599 loss: 0.4392767853159054\n",
      "epoch: 4 iter: 649 loss: 0.4280408170834014\n",
      "epoch: 4 iter: 699 loss: 0.450581477121531\n",
      "epoch: 4 iter: 749 loss: 0.4603508026847052\n",
      "epoch: 4 iter: 799 loss: 0.4875141328131904\n",
      "epoch: 4 iter: 849 loss: 0.4900205970203743\n",
      "epoch: 4 iter: 899 loss: 0.4915161546918932\n",
      "epoch: 4 iter: 949 loss: 0.4764392847897999\n",
      "epoch: 4 iter: 999 loss: 0.44895399348171317\n",
      "epoch: 4 iter: 1049 loss: 0.4485356870621002\n",
      "epoch: 4 iter: 1099 loss: 0.45527129170472347\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128a74b72a59434cb0ff738f72c12049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3951), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 4 val_score: 0.7356213337141545 time: 1102.187315940857\n"
     ]
    }
   ],
   "source": [
    "import editdistance as ed\n",
    "import time\n",
    "import os\n",
    "import nltk.translate.bleu_score as bl\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "CHECKPOINTS = './checkpoints'\n",
    "\n",
    "def compute_bleu_score(model, src_words, trg_words):\n",
    "    return _compute_metric_average(model, src_words, trg_words, lambda x, y: bl.sentence_bleu([list(x)], list(y)))\n",
    "\n",
    "def compute_editdistance(model, src_words, trg_words):\n",
    "    return _compute_metric_average(model, src_words, trg_words, ed.eval)\n",
    "\n",
    "def _compute_metric_average(model, src_words, trg_words, metric):\n",
    "    scs = [metric(model.translate(x[:-5])[:-5], y[:-5]) for x, y in zip(tqdm_notebook(src_words), trg_words)]\n",
    "    return np.mean(scs)\n",
    "\n",
    "def train(model, opt, train_X, train_Y, val_src_words, val_trg_words, checkpoints_folder, metrics_compute_freq=50, n_epochs=7):\n",
    "    cur_loss = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        for i, (x, y) in enumerate(batch_iterator(train_X, train_Y)):\n",
    "            inputs = Variable(torch.from_numpy(x))\n",
    "            targets = Variable(torch.from_numpy(y))\n",
    "            log_predictions = model(inputs, targets)\n",
    "            #print(x)\n",
    "            loss = cross_entropy(log_predictions, targets, be)\n",
    "            #print(loss.data, log_predictions.data.min())\n",
    "            loss.backward()\n",
    "            cur_loss = 0.9 * cur_loss + 0.1 * loss.data[0]\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            if (i + 1) % metrics_compute_freq == 0:\n",
    "                print(\"epoch: {} iter: {} loss: {}\".format(epoch, i, cur_loss))\n",
    "        model.eval() \n",
    "        val_score = compute_bleu_score(model, val_src_words, val_trg_words)\n",
    "        print(\"epoch: {} val_score: {} time: {}\"\n",
    "              .format(epoch, val_score, time.time() - start_time))\n",
    "        torch.save(model.state_dict(), os.path.join(checkpoints_folder, \"state_dict_{}_{}.pth\".format(epoch, val_score)))\n",
    "                \n",
    "train(model, opt, train_X, train_Y, val_src_words, val_trg_words, checkpoints_folder=CHECKPOINTS, metrics_compute_freq=50, n_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMDiscriminator(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, filters_per_ngram=10, ngrams=(3, 7), dense_layer=64):\n",
    "        super(CNNDiscriminator, self).__init__()\n",
    "        self.embedding = nn.Linear(vocab_size, embedding_size)\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv2d(1, filters_per_ngram, (ngram, embedding_size)) for ngram in range(*ngrams)]\n",
    "        )\n",
    "        self.dense1 = nn.Linear(filters_per_ngram * len(self.convs), dense_layer)\n",
    "        self.output = nn.Linear(dense_layer, 1)\n",
    "        \n",
    "    def forward(self, input_sequence):\n",
    "        embedding = F.relu(self.embedding(input_sequence)).unsqueeze(0)\n",
    "        out = F.relu(F.max_pool1d(torch.cat([conv(embedding) for conv in convs])))\n",
    "        out = F.relu(self.dense1(out))\n",
    "        return F.sigmoid(self.output(out))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_discriminator(disc_model, gen_model, opt, alph_X, train_X, train_Y, n_epochs=5):\n",
    "    cur_loss = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        disc_model.train()\n",
    "        gen_model.eval()\n",
    "        start_time = time.time()\n",
    "        for i, (x, y) in enumerate(batch_iterator(train_X, train_Y)):\n",
    "            inputs = Variable(alph_X.get_one_hot_repr(torch.from_numpy(x)))\n",
    "            targets = Variable(torch.from_numpy(y))\n",
    "            real_data_pred = disc_model(targets)\n",
    "            gen_data_pred = disc_model(gen_model(inputs))\n",
    "            loss = nn.BCELoss(gen_data_pred, Variable(torch.zeros_like(gen_data_pred))) \\\n",
    "                    + nn.BCELoss(real_data_pred, Variable(torch.ones_like(real_data_pred)))\n",
    "            cur_loss = 0.9 * cur_loss + 0.1 * loss.data[0]\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            if i % 50 == 49:\n",
    "                print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "disc = CNNDiscriminator(len(be), 65, 10)\n",
    "disc_opt = optim.Adam(disc.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of type Variable[torch.FloatTensor] but found type Variable[torch.LongTensor] for argument #1 'mat1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-3a87688a2185>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisc_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-85-cdb6b1fa9ffa>\u001b[0m in \u001b[0;36mtrain_discriminator\u001b[0;34m(disc_model, gen_model, opt, alph_X, train_X, train_Y, n_epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malph_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_one_hot_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mreal_data_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisc_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mgen_data_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisc_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_data_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_data_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m                     \u001b[0;34m+\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_data_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_data_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-74-b29a75d36e12>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_sequence)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconvs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of type Variable[torch.FloatTensor] but found type Variable[torch.LongTensor] for argument #1 'mat1'"
     ]
    }
   ],
   "source": [
    "train_discriminator(disc, model, disc_opt, ru, train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.translate(\"полемики\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_src_words[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_trg_words[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x, y in zip(val_src_words[:10], val_trg_words):\n",
    "    tr = model.translate(x[:-5])[:-5]\n",
    "    y = y[:-5]\n",
    "    \n",
    "    print(tr, y, ed.eval(tr, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translate(model, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import editdistance as ed\n",
    "\n",
    "\n",
    "scs = []\n",
    "with open(TEST_FILE, \"r\") as ftr:\n",
    "    for ruw, bew in map(lambda x: x.split(), \n",
    "                        filter(lambda x: len(x.split()) == 2, tqdm_notebook(ftr.readlines()[:2000]))):\n",
    "        res = model.translate(ruw)\n",
    "        scs.append(ed.eval(bew, res[:-5]))\n",
    "        print(ruw, bew, res)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! pip3 install --user editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(scs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! head -n 50 data/ru-be.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
