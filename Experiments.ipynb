{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_FILE = 'data/ru-be-train.txt'\n",
    "TEST_FILE = 'data/ru-be-test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 30\n",
    " \n",
    "class Alphabet:\n",
    "    START = '__START__'\n",
    "    END = '_END_'\n",
    " \n",
    "    def __init__(self, max_length=MAX_LENGTH):\n",
    "        self.max_length = max_length\n",
    "        self.letter2index_ = {Alphabet.START : 0, Alphabet.END : 1}\n",
    "        self.index2letter_ = [Alphabet.START, Alphabet.END]\n",
    "        \n",
    "    def get_index(self, letter):\n",
    "        if letter not in self.letter2index_:\n",
    "            self.letter2index_[letter] = len(self.index2letter_)\n",
    "            self.index2letter_.append(letter)\n",
    "        return self.letter2index_[letter]\n",
    "    \n",
    "    @property\n",
    "    def start_index(self):\n",
    "        return self.letter2index_[Alphabet.START]\n",
    "    \n",
    "    @property\n",
    "    def end_index(self):\n",
    "        return self.letter2index_[Alphabet.END]\n",
    "    \n",
    "    def index2letter(self, x):\n",
    "        return ''.join(self.index2letter_[index] for index in x)\n",
    "    \n",
    "    def letter2index(self, word):\n",
    "        lst = [self.get_index(letter) for letter in word]\n",
    "        return lst[:self.max_length - 1] + [self.get_index(Alphabet.END)] * max(1, self.max_length - len(lst))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.index2letter_)\n",
    "    \n",
    "    # torch utils\n",
    "    def get_length(self, input_sequence):\n",
    "        return (input_sequence == self.end_index).max(dim=1)[1] + 1\n",
    "    \n",
    "    def get_mask(self, input_sequence):\n",
    "        return (torch.cumsum(input_sequence == self.end_index, dim=1) < 2).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ru = Alphabet()\n",
    "be = Alphabet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_pair_dataset(filename, alph1, alph2):\n",
    "    x, y = [], []\n",
    "    with open(filename, 'r') as ftr:\n",
    "        for line in ftr:\n",
    "            try:\n",
    "                word1, word2 = line.split()\n",
    "            except ValueError:\n",
    "                continue\n",
    "            x.append(alph1.letter2index(word1))\n",
    "            y.append(alph2.letter2index(word2))\n",
    "    return np.array(x), np.array(y)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X, train_Y = load_pair_dataset(TRAIN_FILE, ru, be)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SimpleGRUEncoder(nn.Module):\n",
    "    def __init__(self, alphabet, embedding_size, hidden_size):\n",
    "        super(SimpleGRUEncoder, self).__init__()\n",
    "        self.alphabet = alphabet\n",
    "        self.embedding = nn.Embedding(num_embeddings=len(self.alphabet), embedding_dim=embedding_size)\n",
    "        self.gru = nn.GRU(input_size=embedding_size, hidden_size=hidden_size, batch_first=True)\n",
    "        \n",
    "    def forward(self, input_sequence):\n",
    "        batch_size = input_sequence.size(0)\n",
    "        embeddings = self.embedding(input_sequence)\n",
    "        out, _ = self.gru(embeddings)\n",
    "        return out[range(batch_size), self.alphabet.get_length(input_sequence) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleGRUDecoder(nn.Module):\n",
    "    def __init__(self, alphabet, embedding_size, hidden_size):\n",
    "        super(SimpleGRUDecoder, self).__init__()\n",
    "        self.alphabet = alphabet\n",
    "        self.embedding = nn.Embedding(num_embeddings=len(alphabet), embedding_dim=embedding_size)\n",
    "        self.gru_cell = nn.GRUCell(input_size=embedding_size, hidden_size=hidden_size)\n",
    "        self.logit_linear = nn.Linear(hidden_size, len(alphabet))\n",
    "        \n",
    "    def forward(self, token, prev_h):\n",
    "        embedding = self.embedding(token)\n",
    "        h = self.gru_cell(embedding, prev_h)\n",
    "        out = self.logit_linear(h)\n",
    "        return out, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SimpleGRUSupervisedSeq2Seq(nn.Module):\n",
    "    def __init__(self, src_alphabet, dst_alphabet, embedding_size, hidden_size):\n",
    "        super(SimpleGRUSupervisedSeq2Seq, self).__init__()\n",
    "        self.encoder = SimpleGRUEncoder(src_alphabet, embedding_size, hidden_size)\n",
    "        self.h_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.decoder = SimpleGRUDecoder(dst_alphabet, embedding_size, hidden_size)\n",
    "        \n",
    "    def start(self, batch_size):\n",
    "        return Variable(torch.from_numpy(np.repeat(self.decoder.alphabet.start_index, batch_size)))\n",
    "    \n",
    "    def forward(self, input_sequence, output_sequence):\n",
    "        enc_h = self.encoder(input_sequence)\n",
    "        dec_h = F.tanh(self.h_linear(enc_h))\n",
    "        logits = []\n",
    "        for x in itertools.chain((self.start(output_sequence.size(0)),), output_sequence.transpose(0, 1)[:-1]):\n",
    "            out, dec_h = self.decoder(x, dec_h)\n",
    "            logits.append(out)\n",
    "        return F.log_softmax(torch.stack(logits, dim=1), dim=-1)\n",
    "    \n",
    "    def translate(self, word, strategy='', max_length=30):\n",
    "        self.eval()\n",
    "        input_sequence = Variable(torch.from_numpy(np.array([self.encoder.alphabet.letter2index(word)])))\n",
    "        #print(input_sequence.shape)\n",
    "        hidden = F.tanh(self.h_linear(self.encoder(input_sequence)))\n",
    "        token = self.start(1)\n",
    "        #print(token.shape, hidden.shape)\n",
    "        lst = []\n",
    "        for i in range(10):\n",
    "            out, hidden = self.decoder(hidden, hidden)\n",
    "            token = out.max(1)[1]\n",
    "            #print(token, out)\n",
    "            lst.append(token.data[0])\n",
    "            if token.data[0] == self.decoder.alphabet.end_index:\n",
    "                break\n",
    "        return ''.join(self.decoder.alphabet.index2letter(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iterator(X, Y=None, batch_size=32):\n",
    "    assert Y is None or X.shape[0] == Y.shape[0]\n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        if Y is not None:\n",
    "            yield X[i:i + batch_size], Y[i:i + batch_size]\n",
    "        else:\n",
    "            yield X[i:i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SimpleGRUSupervisedSeq2Seq(ru, be, 20, 128)\n",
    "opt = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.4267039775848389\n",
      "50 3.017125253379641\n",
      "100 2.8566916491802283\n",
      "150 2.6569024035058244\n",
      "200 2.5558981824584874\n",
      "250 2.4629374067439977\n",
      "300 2.448218905982744\n",
      "350 2.408644325049544\n",
      "400 2.3337143830146885\n",
      "450 2.2942711534909717\n",
      "500 2.2577187371134197\n",
      "550 2.225521624412744\n",
      "600 2.191419548497916\n",
      "650 2.1366446513904354\n",
      "700 2.096031303106419\n",
      "750 2.0582389565804435\n",
      "800 1.9783964410218875\n",
      "850 1.9377282285932376\n",
      "900 1.903832688812762\n",
      "950 1.8791217350708709\n",
      "1000 1.8393332935364415\n",
      "1050 1.817865797141295\n",
      "1100 1.7627719991147452\n",
      "1150 1.7399911065736364\n",
      "1200 1.7073639922944785\n",
      "1250 1.6991308142751853\n",
      "1300 1.631982261550076\n",
      "1350 1.646041470315422\n",
      "1400 1.6432528764843082\n",
      "1450 1.5972227410757223\n",
      "1500 1.6097965745619114\n",
      "1550 1.5725634690439907\n",
      "1600 1.4905670423466602\n",
      "1650 1.4828901499001503\n",
      "1700 1.5043077476634845\n",
      "1750 1.4790364644010026\n",
      "1800 1.384018368218199\n",
      "1850 1.3745882533296847\n",
      "0 1.3620776218242419\n",
      "50 1.3671799249667576\n",
      "100 1.3361650960330826\n",
      "150 1.3489249794967286\n",
      "200 1.34003538274784\n",
      "250 1.2771855924592983\n",
      "300 1.2814455883920581\n",
      "350 1.3309585325956719\n",
      "400 1.3011551162876032\n",
      "450 1.3083184689826264\n",
      "500 1.2835813557028806\n",
      "550 1.2423211448108484\n",
      "600 1.2739279963071988\n",
      "650 1.1984025438009027\n",
      "700 1.2123368630789972\n",
      "750 1.262656800246209\n",
      "800 1.1624003113017884\n",
      "850 1.1572536962613353\n",
      "900 1.138246609414786\n",
      "950 1.1889704141056245\n",
      "1000 1.148928156165446\n",
      "1050 1.1667786372075994\n",
      "1100 1.1285207638018475\n",
      "1150 1.1430998200378963\n",
      "1200 1.1231576875998548\n",
      "1250 1.1499246930965545\n",
      "1300 1.0751020605324852\n",
      "1350 1.1181026530453702\n",
      "1400 1.1520772603652274\n",
      "1450 1.114150251621685\n",
      "1500 1.1692552466577257\n",
      "1550 1.111523252106854\n",
      "1600 1.0523255370630473\n",
      "1650 1.0798110958339906\n",
      "1700 1.0962364736960082\n",
      "1750 1.0989808419505267\n",
      "1800 1.00805922904889\n",
      "1850 0.9979339940242008\n",
      "0 0.9731431286873966\n",
      "50 1.005069575531186\n",
      "100 0.9813894296187247\n",
      "150 1.0067033127556795\n",
      "200 1.0047531124058975\n",
      "250 0.9584962810374423\n",
      "300 0.9563995570787723\n",
      "350 1.0344769202668145\n",
      "400 1.0051363998447447\n",
      "450 1.0144211818045654\n",
      "500 0.981869281077073\n",
      "550 0.9533804199868117\n",
      "600 1.014700388943309\n",
      "650 0.9337429704624793\n",
      "700 0.9616299435087597\n",
      "750 1.0104431550580169\n",
      "800 0.9159829077008597\n",
      "850 0.9273823866469617\n",
      "900 0.9027396377944763\n",
      "950 0.9667170614729232\n",
      "1000 0.9285876808940544\n",
      "1050 0.9645966659450835\n",
      "1100 0.9341399165194426\n",
      "1150 0.9454763173767635\n",
      "1200 0.928208344760902\n",
      "1250 0.9643816362087246\n",
      "1300 0.8918140791360087\n",
      "1350 0.9457330527090879\n",
      "1400 0.9784711334194616\n",
      "1450 0.9433052312109059\n",
      "1500 0.9983291538683736\n",
      "1550 0.9264925391509489\n",
      "1600 0.8900529280529168\n",
      "1650 0.9284058222790813\n",
      "1700 0.9380396558734002\n",
      "1750 0.9153007178258636\n",
      "1800 0.8352385743237409\n",
      "1850 0.8240676490601518\n",
      "0 0.7953601149907371\n",
      "50 0.8315944766619618\n",
      "100 0.8209729214488329\n",
      "150 0.8379358279748073\n",
      "200 0.8463079714396915\n",
      "250 0.811500897884208\n",
      "300 0.8025441822258415\n",
      "350 0.8837509571657265\n",
      "400 0.8593787393342602\n",
      "450 0.8666287655870093\n",
      "500 0.8340312016486211\n",
      "550 0.807681619492201\n",
      "600 0.8804054744325815\n",
      "650 0.8063661702316541\n",
      "700 0.8274388203744812\n",
      "750 0.8835629321860022\n",
      "800 0.7954125773845399\n",
      "850 0.8123933563782046\n",
      "900 0.7865556818704831\n",
      "950 0.8589267439658013\n",
      "1000 0.8118588009825829\n",
      "1050 0.8457928022431458\n",
      "1100 0.8229145553698608\n",
      "1150 0.8372756896234228\n",
      "1200 0.8210647623935112\n",
      "1250 0.8585853111638796\n",
      "1300 0.7904319648929468\n",
      "1350 0.8530628704019382\n",
      "1400 0.8800219936958947\n",
      "1450 0.8459025188754166\n",
      "1500 0.8996577104642858\n",
      "1550 0.8280548763864255\n",
      "1600 0.7943740547453574\n",
      "1650 0.8320801110249042\n",
      "1700 0.8454986424433827\n",
      "1750 0.8001826313225597\n",
      "1800 0.7291953480255433\n",
      "1850 0.7124241853918298\n",
      "0 0.6853586261256723\n",
      "50 0.7318580719809182\n",
      "100 0.7215543875213289\n",
      "150 0.733646084854584\n",
      "200 0.7381206168738297\n",
      "250 0.7135279555392132\n",
      "300 0.7059868521350874\n",
      "350 0.7775169349242663\n",
      "400 0.7590464698639947\n",
      "450 0.7704769138705135\n",
      "500 0.7357937703850237\n",
      "550 0.7132532892943964\n",
      "600 0.7904111159251592\n",
      "650 0.7338760163597108\n",
      "700 0.7424676756296434\n",
      "750 0.8029285257157641\n",
      "800 0.7092054673438584\n",
      "850 0.7385727032893528\n",
      "900 0.7043444900904726\n",
      "950 0.7801581790424152\n",
      "1000 0.7344123843343406\n",
      "1050 0.7618950781672867\n",
      "1100 0.7432563461070286\n",
      "1150 0.7638128473313336\n",
      "1200 0.752203395313812\n",
      "1250 0.7863886374998715\n",
      "1300 0.7233639648241494\n",
      "1350 0.7925213578980318\n",
      "1400 0.8100617170413433\n",
      "1450 0.7825342878014621\n",
      "1500 0.833012880333108\n",
      "1550 0.7607951574685133\n",
      "1600 0.7371608869473751\n",
      "1650 0.7654323965236172\n",
      "1700 0.7812732635489031\n",
      "1750 0.7129839762421506\n",
      "1800 0.6542384493112223\n",
      "1850 0.6354310410584143\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy(log_predictions, targets, alphabet):\n",
    "    length_mask = alphabet.get_mask(targets)\n",
    "    targets_mask = torch.zeros_like(log_predictions).scatter_(2, targets.view(*targets.shape, 1), 1.0)\n",
    "    mask = targets_mask * length_mask.view(*length_mask.shape, 1)\n",
    "    return (log_predictions * mask / (mask.sum(1, keepdim=True).sum(2, keepdim=True) * -log_predictions.size(0))).sum()\n",
    " \n",
    "cur_loss = 0\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    for i, (x, y) in enumerate(batch_iterator(train_X, train_Y)):\n",
    "        inputs = Variable(torch.from_numpy(x))\n",
    "        targets = Variable(torch.from_numpy(y))\n",
    "        log_predictions = model(inputs, targets)\n",
    "        loss = cross_entropy(log_predictions, targets, be)\n",
    "        #print(loss.data, log_predictions.data.min())\n",
    "        loss.backward()\n",
    "        cur_loss = 0.9 * cur_loss + 0.1 * loss.data[0]\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        if i % 50 == 0:\n",
    "            print(i, cur_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  4, 15,  9, 12,  7, 14, 12,  3,  4, 18,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "       [12,  7,  8,  8,  9, 11, 13,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "       [25,  9,  4,  3, 14,  5, 28,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "       [11, 10,  3, 21, 13, 12,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "       [10,  9,  4,  8, 18, 15,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "       [ 2,  4,  3, 12,  3, 12,  9,  2,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "       [ 5, 20,  5, 10,  5,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "       [29, 12,  3, 16,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "       [21,  4, 13, 26,  3, 19, 18, 15,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "       [17, 22,  5,  8, 21,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "       [11,  9,  8, 12,  7, 26,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "       [ 2,  7,  4, 19,  3,  8,  5, 29,  5, 22, 23,  8, 18, 24,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def translate(model, word, strategy='', max_length=30):\n",
    "    model.eval()\n",
    "    input_sequence = Variable(torch.from_numpy(np.array([model.encoder.alphabet.letter2index(word)])))\n",
    "    #print(input_sequence.shape)\n",
    "    hidden = F.tanh(model.h_linear(model.encoder(input_sequence)))\n",
    "    token = model.start(1)\n",
    "    #print(token.shape, hidden.shape)\n",
    "    lst = []\n",
    "    for i in range(10):\n",
    "        out, hidden = model.decoder(token, hidden)\n",
    "        token = out.max(1)[1]\n",
    "        #print(token, out)\n",
    "        lst.append(token.data[0])\n",
    "        if token.data[0] == model.decoder.alphabet.end_index:\n",
    "            break\n",
    "    return ''.join(model.decoder.alphabet.index2letter(lst))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'дрэва_END_'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model, \"дерево\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "поражением\tпаразай\r\n",
      "местному\tмясцовым\r\n",
      "испанских\tіспанскіх\r\n",
      "сорока\tсарока\r\n",
      "способна\tздольная\r\n",
      "факторы\tфактары\r\n",
      "высадки\tвысадкі\r\n",
      "аргентина\tаргенціна\r\n",
      "феликс\tфелікс\r\n",
      "финальном\tфінальным\r\n",
      "фактором\tфактарам\r\n",
      "оригинального\tарыгінальнага\r\n",
      "древнейших\tнайстаражытных\r\n",
      "художественным\tмастацкім\r\n",
      "резиденции\tрэзідэнцыі\r\n",
      "кораблях\tкараблях\r\n",
      "православие\tправаслаўе\r\n",
      "мл\tмл\r\n",
      "гражданский\tграмадзянскі\r\n",
      "транспортного\tтранспартнага\r\n",
      "листьями\tлісцем\r\n",
      "поврежд\tповрежд\r\n",
      "выехал\tвыехаў\r\n",
      "сицилии\tсіцыліі\r\n",
      "чтение\tчытанне\r\n",
      "повсеместно\tпаўсюдна\r\n",
      "украшения\tўпрыгажэнні\r\n",
      "передали\tперадалі\r\n",
      "проявления\tпраявы\r\n",
      "вокалист\tвакаліст\r\n",
      "склад\tсклад\r\n",
      "уголовное\tкрымінальная\r\n",
      "работало\tпрацавала\r\n",
      "ушел\tсышоў\r\n",
      "запросов\tзапытаў\r\n",
      "спектакля\tспектакля\r\n",
      "продать\tпрадаць\r\n",
      "сл\tсл\r\n",
      "ленинского\tленінскага\r\n",
      "землями\tземлямі\r\n",
      "свидетелей\tсведак\r\n",
      "существующей\tіснуючай\r\n",
      "частного\tпрыватнага\r\n",
      "образцу\tузоры\r\n",
      "отмечали\tадзначалі\r\n",
      "воздействием\tуздзеяннем\r\n",
      "категорически\tкатэгарычна\r\n",
      "дальнейшие\tдалейшыя\r\n",
      "сообщается\tпаведамляецца\r\n",
      "периоде\tперыядзе\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 50 data/ru-be.txt"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
